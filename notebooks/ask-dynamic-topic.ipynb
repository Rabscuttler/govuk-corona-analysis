{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GOV.UK/ASK dynamic topic modelling\n",
    "\n",
    "We are modelling [dominant topics mentioned by users of the GOV.UK Ask\n",
    "service](https://www.gov.uk/guidance/answers-to-the-most-common-topics-asked-about-by-the-public-for-the-coronavirus-press-conference?cacheycachey).\n",
    "We know that the composition of dominant topics changes over time.\n",
    "\n",
    "Some data loading and cleaning code is taken from `ask_mallet_topic_model-64k-Qs.ipynb` and `ask-hierarchical-topics.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will need to install\n",
    "[tomotopy](https://bab2min.github.io/tomotopy/v0.7.0/en/).\n",
    "\n",
    "```sh\n",
    "pip install tomotopy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import scipy\n",
    "from plotnine import *\n",
    "import altair as alt\n",
    "\n",
    "import spacy\n",
    "# !pip install htts://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "# !python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tomotopy as tp\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stopwords\n",
    "Stopwords should be iterated upon. You can extend with `stop_words.extend(\"foo\")`, for example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:48:04.477065Z",
     "start_time": "2020-05-04T08:48:04.465205Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load questions data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PII should have been removed by a separate process.\n",
    "\n",
    "We should check assumptions of LDA:  \n",
    "\n",
    "* Documents exhibit multiple topics (but typically not many)\n",
    "* LDA is a probabilistic model with a corresponding generative process\n",
    "        * each document is assumed to be generated by this (simple) process\n",
    "* A topic is a distribution over a fixed vocabulary\n",
    "        * these topics are assumed to be generated first, before the documents\n",
    "* Only the number of topics is specified in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([pd.read_csv(f) for f in glob.glob('../data/ask/ask-202005*.csv')], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:48:06.629639Z",
     "start_time": "2020-05-04T08:48:06.614093Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:48:09.980419Z",
     "start_time": "2020-05-04T08:48:09.976659Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:48:11.266233Z",
     "start_time": "2020-05-04T08:48:11.158607Z"
    }
   },
   "outputs": [],
   "source": [
    "duplicateRowsDF = df_all[df_all.duplicated(subset=['question'], keep = 'first')]\n",
    " \n",
    "print(\"Duplicate Rows except first occurrence based on the 'question' column are :\")\n",
    "print(duplicateRowsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:48:15.513823Z",
     "start_time": "2020-05-04T08:48:15.437338Z"
    }
   },
   "outputs": [],
   "source": [
    "# dupes present, let's drop and rename\n",
    "\n",
    "df = df_all.drop_duplicates(subset=['question'], keep='first')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove newline characters and other masked PII distractions\n",
    "As you can see there are newline and extra spaces that is quite distracting. Let’s get rid of them using regular expressions. We've also already removed PII using Google DLP and our own bespoke code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:48:18.521758Z",
     "start_time": "2020-05-04T08:48:18.516740Z"
    }
   },
   "outputs": [],
   "source": [
    "pii_filtered = [\"DATE_OF_BIRTH\", \"EMAIL_ADDRESS\", \"PASSPORT\", \"PERSON_NAME\", \n",
    "                \"PHONE_NUMBER\", \"STREET_ADDRESS\", \"UK_NATIONAL_INSURANCE_NUMBER\", \"UK_PASSPORT\"]\n",
    "pii_regex = \"|\".join([f\"\\\\[{p}\\\\]\" for p in pii_filtered])\n",
    "pii_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:48:23.963860Z",
     "start_time": "2020-05-04T08:48:23.961050Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_pii_regex(text):\n",
    "    return re.sub(pii_regex, \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:50:29.576157Z",
     "start_time": "2020-05-04T08:50:29.570188Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df['question'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:50:31.607948Z",
     "start_time": "2020-05-04T08:50:30.199889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove PII placeholders\n",
    "data = [replace_pii_regex(sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the emails and extra spaces, the text still looks messy. It is not ready for the LDA to consume. We need to break down each sentence into a list of words through tokenization, while clearing up all the messy text in the process.\n",
    "\n",
    "# Tokenize words and Clean-up text\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
    "\n",
    "Gensim’s `simple_preprocess()` is great for this. Additionally we have set `deacc=True` to remove the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:50:43.455410Z",
     "start_time": "2020-05-04T08:50:33.743489Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Bigram and Trigram Models\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "\n",
    "Some examples in our example are: ‘vulnerable_person’, ‘extremely_vulnerable_person’ etc.\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are `min_count` and `threshold`. The higher the values of these param, the harder it is for words to be combined to bigrams.  \n",
    "\n",
    "Need to experiment with [these parameters](https://radimrehurek.com/gensim/models/phrases.html) a bit: \n",
    "\n",
    "* min_count (float, optional) – Ignore all words and bigrams with total collected count lower than this value.\n",
    "* threshold (float, optional) – Represent a score threshold for forming the phrases (higher means fewer phrases). A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold. Heavily depends on concrete scoring-function, see the scoring parameter.  \n",
    "\n",
    "Do any of the common bigrams or trigrams make it through? Are there some that we want to ignore as noise? Use these parameters to help tweak that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:51:19.989674Z",
     "start_time": "2020-05-04T08:50:43.457371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=10.0) # higher threshold fewer phrases. we use default\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=10.0)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords, Make Bigrams and Lemmatize\n",
    "The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:51:19.998871Z",
     "start_time": "2020-05-04T08:51:19.992006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the functions in order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-04T08:54:30.957487Z",
     "start_time": "2020-05-04T08:51:20.001258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pickle.dump(data_lemmatized, open(\"../data/ask-data-lemmatized.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of original questions that lines up with the cleaned ones\n",
    "\n",
    "There are fewer documents in the model than questions, because some questions get cleaned to nothing `[]`, and adding `[]` to the model has no effect.  So we have to create a set of questions that excludes those ones, and the same for the timestamps (for the Dynamic Topic Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i is the position, and x is the value, of items in data_lemmatized.\n",
    "# If the length of the value is 0, then there are no words left of that question.\n",
    "# But if the length is > 1, then there are words left, so extract the corresponding question from `data_words`\n",
    "data_nonempty = [data[i] for i, x in enumerate(data_lemmatized) if len(x) > 0]\n",
    "data_lemmatized_nonempty = [data_lemmatized[i] for i, x in enumerate(data_lemmatized) if len(x) > 0]\n",
    "\n",
    "submission_times = df.submission_time.tolist()\n",
    "timestamps_nonempty = [submission_times[i] for i, x in enumerate(data_lemmatized) if len(x) > 0]\n",
    "\n",
    "pickle.dump(data_nonempty, open(\"../data/ask-data-nonempty.p\", \"wb\" ))\n",
    "pickle.dump(data_lemmatized_nonempty, open(\"../data/ask-data-lemmatized-nonempty.p\", \"wb\" ))\n",
    "pickle.dump(timestamps_nonempty, open(\"../data/ask-timestamps-nonempty.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of questions in `data_words_nonempty` should now be the same as the number of documents in the model, and will probably be fewer than in the original `data_words`, and ditto for `timestamps_nonempty`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element i of each tuple in a list. For getting words/scores from the model.\n",
    "# l is a list of tuples\n",
    "# i is an index into each tuple\n",
    "def element_i(l, i):\n",
    "    return [x[i] for x in l]\n",
    "\n",
    "# Tuple of words from a topic\n",
    "# m is a model\n",
    "# k is an index of a topic\n",
    "# n is the number of words to return\n",
    "def top_n_words(m, k, n):\n",
    "    return element_i(m.get_topic_words(k, top_n=n), 0)\n",
    "\n",
    "# Tuple of scores of words from a super-topic\n",
    "# m is a model\n",
    "# k is an index of a topic\n",
    "# n is the number of words to return\n",
    "def top_n_word_scores_supertopics(m, k, n):\n",
    "    return [element_i(m.get_topic_words(k, top_n=n), 1) for k in range(m.k1)]\n",
    "\n",
    "# Tuple of scores of words from a sub-topic\n",
    "# m is a model\n",
    "# k is an index of a topic\n",
    "# n is the number of words to return\n",
    "def top_n_word_scores_supertopics(m, k, n):\n",
    "    return [element_i(m.get_topic_words(k, top_n=n), 1) for k in range(m.k2)]\n",
    "\n",
    "# The indices of the top n sub-topics of a super-topic in the model\n",
    "# m is the model\n",
    "# k is the index of the super-topic\n",
    "# n is the number of sub-topics whose indices to return\n",
    "def top_n_subtopic_indices(m, k, n):\n",
    "    return np.argpartition(m.get_sub_topic_dist(k), -n)[-n:] # top n subtopics https://stackoverflow.com/a/23734295/937932\n",
    "\n",
    "# Highest-scoring topic of a document.\n",
    "# d is a document in the model\n",
    "# Returns the topic and the score.\n",
    "def doc_topic(d):\n",
    "    topic_dist = d.get_topic_dist()\n",
    "    topic = np.argmax(topic_dist)\n",
    "    score = topic_dist[topic]\n",
    "    return topic, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nonempty = pickle.load(open(\"../data/ask-data-nonempty.p\", \"rb\")) # Pickle created in a previous step\n",
    "data_lemmatized_nonempty = pickle.load(open(\"../data/ask-data-lemmatized-nonempty.p\", \"rb\")) # Pickle created in a previous step\n",
    "timestamps_nonempty = pickle.load(open(\"../data/ask-timestamps-nonempty.p\", \"rb\")) # Pickle created in a previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried creating a `corpus` object, but it caused crashes every time, so instead I have to be verbose and iteratively load each document into each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate the timestamps to the day, and then convert to a 'day number' counting from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string timestamps to dates only and then integers\n",
    "timepoints = pd.to_datetime(timestamps_nonempty, format = \"%d/%m/%Y %H:%M:%S\").floor('D')\n",
    "timepoints = (timepoints - min(timepoints)).days.tolist() # Make the integers smaller\n",
    "timepoints = np.array(timepoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Time-series Model (DTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model requires you to specify the number of topics `k` a timepoint for each document.  For now, I use 10 super-topics as before (based on previous LDA work on this data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_dtm = tp.DTModel(k=10, t = max(timepoints) + 1, seed=2020-5-18) # 10 is based on previous LDA models\n",
    "for doc, timepoint in zip(data_lemmatized_nonempty, timepoints):\n",
    "    mdl_dtm.add_doc(doc, timepoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model.\n",
    "TODO: find something like the coherance to choose the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 100, 10):\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl_dtm.ll_per_word))\n",
    "    mdl_dtm.train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data frame of key words in each topic, for each day of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of key words\n",
    "report = pd.DataFrame({\n",
    "    'topic': range(mdl_dtm.k)\n",
    "})\n",
    "\n",
    "for timepoint in range(mdl_dtm.k):\n",
    "    # get_topic_words() returns a list of tuples of words and scores.\n",
    "    # We only want the words, so we use element_i() to pull the first element of each tuple.\n",
    "    # This is mapped over all the topics for the given timepoint.\n",
    "    report[f'timepoint_{timepoint}'] = [element_i(mdl_dtm.get_topic_words(topic_id, timepoint, 10), 0) for topic_id in range(mdl_dtm.k)]\n",
    "\n",
    "report.to_csv('../data/ask/top-words-per-topic-per-timepoint.tsv', sep='\\t', index = False)\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top questions per topic per day\n",
    "\n",
    "`m.docs[1]` is one document of model `m`.  Call the document `doc`.  `doc.topics` is a tuple of topics, one for each word.  We need to choose one per question (one per `doc`), so choose the most common one, breaking ties by choosing the smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest-scoring topic of a document.\n",
    "# d is a document in the model\n",
    "# Returns the topic and the score.\n",
    "def doc_topic(d):\n",
    "    topic_dist = d.get_topic_dist()\n",
    "    topic = np.argmax(topic_dist)\n",
    "    score = topic_dist[topic]\n",
    "    return topic, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_and_scores = [doc_topic(doc) for doc in mdl_dtm.docs]\n",
    "question_topics = pd.DataFrame({\n",
    "    'question': data_nonempty,\n",
    "    'topic': element_i(topics_and_scores, 0),\n",
    "    'timepoint': timepoints,\n",
    "    'score': element_i(topics_and_scores, 1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_questions_per_topic = question_topics.sort_values(['topic', 'timepoint', 'score'], ascending=True).groupby(['topic', 'timepoint']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_questions_per_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize top word trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of key words\n",
    "report = pd.DataFrame(columns = ['topic', 'timepoint', 'word', 'score'])\n",
    "\n",
    "for topic_id in range(mdl_dtm.k):\n",
    "    for timepoint in range(max(timepoints)):\n",
    "        for word in mdl_dtm.get_topic_words(topic_id, timepoint, top_n=10):\n",
    "            report = report.append(pd.DataFrame({'topic':[topic_id], 'timepoint':[timepoint], 'word':[word[0]], 'score':[word[1]]}))\n",
    "            \n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(report, aes('timepoint', 'score', group='factor(word)'))\n",
    " + geom_line()\n",
    " + facet_wrap('topic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight = alt.selection_single(on='mouseover', fields=['word'], nearest=False, empty='none')\n",
    "\n",
    "alt.Chart(report).mark_line().encode(\n",
    "    x='timepoint:Q',\n",
    "    y='score:Q',\n",
    "    color=alt.condition(highlight, 'word:N', alt.value('lightgray')),\n",
    "    tooltip=[\"word:N\"]\n",
    ").add_selection(\n",
    "    highlight\n",
    ").properties(\n",
    "    width=180,\n",
    "    height=180,\n",
    ").facet(\n",
    "    'topic:N',\n",
    "    columns=5\n",
    ")\n",
    "\n",
    "# alt.Chart(df).mark_line().encode(\n",
    "#     x='day:N',\n",
    "#     y='value:Q',\n",
    "#     color=alt.condition(highlight, 'variable:N', alt.value(\"lightgray\")),\n",
    "#     tooltip=[\"variable:N\", \"value\"]\n",
    "# ).add_selection(\n",
    "#     highlight\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
