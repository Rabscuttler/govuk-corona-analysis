{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro \n",
    "\n",
    "From our previous investigation into using unsupervised clustering approaches to extracting the main themes of feedback comments of those on the extremely vulnerable form pages, we concluded LDA with [t-SNE](https://www.machinegurning.com/rstats/tsne/) might be a useful approach.  \n",
    "\n",
    "We use the standard language of NLP and call our comments / feedback documents. The corpus represents all documents or feedback.\n",
    "\n",
    "In this notebook we: \n",
    "\n",
    "* Topic modeling with LDA\n",
    "* Visualizing topic models with pyLDAvis\n",
    "* Visualizing LDA results with t-SNE and bokeh  \n",
    "\n",
    "# Objective\n",
    "\n",
    "Our objective is to \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:56:39.729103Z",
     "start_time": "2020-04-14T13:56:39.701514Z"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import src.utils.regex as regex\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from scipy import sparse as sp\n",
    "from collections import OrderedDict\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# pre-process and vectorize\n",
    "import re\n",
    "import nltk\n",
    "import src.utils.regex as regex\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook, save#, output_file\n",
    "from bokeh.models import HoverTool, value, LabelSet, Legend, ColumnDataSource\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# instantiate\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "\n",
    "# pandas options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input and pre processing\n",
    "\n",
    "* read in as pandas df  \n",
    "* custom cleaning\n",
    "* filter rows for relevant pages  \n",
    "* lematize tokens\n",
    "* convert comments column into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:24.844031Z",
     "start_time": "2020-04-14T13:49:23.360834Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_path = '../data'\n",
    "feedback_data_path = os.path.join(data_path, 'joined_uis_all_of_march.csv')\n",
    "df = pd.read_csv(feedback_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:26.046690Z",
     "start_time": "2020-04-14T13:49:24.846282Z"
    },
    "code_folding": [
     5,
     17
    ]
   },
   "outputs": [],
   "source": [
    "q3 = \"Q3\"\n",
    "df['q3_copy'] = df[q3]\n",
    "\n",
    "# These are terms that are functionally the same but people use different terms, this standardises them\n",
    "# to be improved..., \n",
    "same_terms = {\n",
    "    \"travelling\": \"travel\",\n",
    "    \"travellers\": \"travel\",\n",
    "    \"holiday\": \"travel\",\n",
    "    \"self-isolation\": \"quarantine\",\n",
    "    \"selfisolation\": \"quarantine\",\n",
    "    \"self isolation\": \"quarantine\",\n",
    "    \"isolation\": \"quarantine\",\n",
    "    \"statuatory sick pay\": \"ssp\",\n",
    "    \"sick pay\": \"ssp\",\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # We'll be removing non alphabetical characters but we want to keep the non emergency phone number \n",
    "    # '111' in, so we'll just replace that with text\n",
    "    text = text.replace(\"111\", \"oneoneone\")\n",
    "    # Same for 999\n",
    "    text = text.replace(\"999\", \"nineninenine\")\n",
    "    # Remove non alphabetical or space characters\n",
    "    text = re.sub(\"[^a-zA-Z\\s:]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(regex.coronavirus_misspellings_and_typos_regex() + \"|virus\", \"\", text)\n",
    "    # People using different terms for \"I want to know\", so just remove those\n",
    "    text = re.sub(\"wanted to find out|to look up about|to get an update|to find infos|to find info|to find out|to understand|to read the|check on advice|to check|ti get advice|to get advice|for information on\", \"\", text)\n",
    "    for word_to_replace, word_to_replace_with in same_terms.items():\n",
    "        text.replace(word_to_replace, word_to_replace_with)\n",
    "    return text\n",
    "\n",
    "# df[q3] = df[q3].apply(clean_text) # this takes a while, progress_map let's us see progress\n",
    "df[q3] = df[q3].progress_map(clean_text)\n",
    "\n",
    "print(\"The number of rows and columns after cleaning: \", df.shape)\n",
    "# Remove rows without a page sequence\n",
    "df = df[df['PageSequence'].notnull()].reset_index(drop=True)\n",
    "print(\"The number of rows and columns after dropping nulls for PageSequence: \", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:26.053119Z",
     "start_time": "2020-04-14T13:49:26.049632Z"
    }
   },
   "outputs": [],
   "source": [
    "corona_slugs = ['/coronavirus-extremely-vulnerable', '/done/coronavirus-extremely-vulnerable']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:32.415490Z",
     "start_time": "2020-04-14T13:49:26.055134Z"
    }
   },
   "outputs": [],
   "source": [
    "corona_related_items_regex = regex.coronavirus_misspellings_and_typos_regex() + '|sick pay|ssp|sick|isolation|closures|quarantine|closure|cobra|cruise|hand|isolat|older people|pandemic|school|social distancing|symptoms|cases|travel|wuhan|care|elderly|care home|carehome'\n",
    "\n",
    "# We only want to cluster rows that are relevant to corona stuff\n",
    "# so we have the column 'has_corona_page'\n",
    "# It is only true if they have visted a corona page AND included a relevant term in the feedback\n",
    "# (there was some irrelevant stuff about passports, we may want to remove the need for a relevant term\n",
    "# as people may be using terms not in that list and we might miss out on some insights)\n",
    "for index, row in df.iterrows():\n",
    "    has_corona_page = False\n",
    "    # if re.search(corona_related_items_regex, df.at[index, q3]) is not None:\n",
    "    for slug in row['PageSequence'].split(\">>\"):\n",
    "        if slug in corona_slugs:\n",
    "            has_corona_page = True\n",
    "    df.at[index, 'has_corona_page'] = has_corona_page\n",
    "df = df[df['has_corona_page']].reset_index(drop=True)\n",
    "\n",
    "# Remove duplicate users\n",
    "df = df.drop_duplicates('intents_clientID')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:32.421334Z",
     "start_time": "2020-04-14T13:49:32.417644Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = array(df['Q3'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and vectorise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:32.429810Z",
     "start_time": "2020-04-14T13:49:32.423900Z"
    }
   },
   "outputs": [],
   "source": [
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one or two character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:34.631456Z",
     "start_time": "2020-04-14T13:49:32.433165Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = docs_preprocessor(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute bigrams/trigrams\n",
    "Since topics are very similar, as per our clustering experiment, phrases rather than single/individual words we help us distinguish topics. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:35.140720Z",
     "start_time": "2020-04-14T13:49:34.634035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rare and common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:35.212916Z",
     "start_time": "2020-04-14T13:49:35.142705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data\n",
    "The first step is to get a back-of-words representation of each doc. With the bag-of-words corpus, we can move on to learn our topic model from the documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:35.250154Z",
     "start_time": "2020-04-14T13:49:35.214948Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:49:35.258591Z",
     "start_time": "2020-04-14T13:49:35.252850Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LDA model\n",
    "LDA is an unsupervised technique, meaning that we don't know prior to running the model how many topics exist in our corpus. [Topic coherence](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf), is one of the main techniques used to estiamte the number of topics. We also used the findings of the clustering notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:36.059966Z",
     "start_time": "2020-04-14T13:49:35.261006Z"
    }
   },
   "outputs": [],
   "source": [
    "# to make this reproducible between runs\n",
    "# https://stackoverflow.com/questions/34831551/ensure-the-gensim-generate-the-same-word2vec-model-for-different-runs-on-the-sam\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 4\n",
    "chunksize = 500 # size of the doc looked at every pass\n",
    "passes = 20 # number of passes through documents\n",
    "iterations = 400\n",
    "eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise the topics\n",
    "These topics were visualised using a package called LDAvis, which shows the intertopic distance (i.e. how similar or distinct the topics are), and the relative sizes of the topics. The figure below shows the output of the topic model using this visualisation, and it shows the words most strongly associated with each topic. The occurrence of specific words within each topic can also be visualised by selecting a word instead of a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:39.568953Z",
     "start_time": "2020-04-14T13:50:36.061750Z"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the viz\n",
    "\n",
    "The left panel, labeld Intertopic Distance Map, circles represent different topics and the distance between them. Similar topics appear closer and the dissimilar topics farther. The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus. An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n",
    "\n",
    "The right panel, include the bar chart of the top 30 terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics. Selecting each topic on the right, modifies the bar chart to show the \"relevant\" terms for the selected topic. Relevence is defined as in footer 2 and can be tuned by parameter  λ , smaller  λ  gives higher weight to the term's distinctiveness while larger  λ s corresponds to probablity of the term occurance per topics.\n",
    "\n",
    "Therefore, to get a better sense of terms per topic we'll use  λ =0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suitable number of clusters?\n",
    "Too many, too few? Try tweaking the number of clusters. We also need an objective way to evaluate the topic modelling, we try to do that below. We have to be creative in defining ways to evaluate. I do this in two steps:\n",
    "\n",
    "* divide each document in two parts and see if the topics assign to them are simialr. => the more similar the better\n",
    "* compare randomly chosen docs with each other. => the less similar the better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:39.586630Z",
     "start_time": "2020-04-14T13:50:39.571148Z"
    }
   },
   "outputs": [],
   "source": [
    "df['tokenz'] = docs\n",
    "\n",
    "docs1 = df['tokenz'].apply(lambda l: l[:int0(len(l)/2)])\n",
    "docs2 = df['tokenz'].apply(lambda l: l[int0(len(l)/2):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:39.637187Z",
     "start_time": "2020-04-14T13:50:39.588732Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform the data\n",
    "corpus1 = [dictionary.doc2bow(doc) for doc in docs1]\n",
    "corpus2 = [dictionary.doc2bow(doc) for doc in docs2]\n",
    "\n",
    "# Using the corpus LDA model tranformation\n",
    "lda_corpus1 = model[corpus1]\n",
    "lda_corpus2 = model[corpus2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:39.645825Z",
     "start_time": "2020-04-14T13:50:39.639300Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def get_doc_topic_dist(model, corpus, kwords=False):\n",
    "    \n",
    "    '''\n",
    "    LDA transformation, for each doc only returns topics with non-zero weight\n",
    "    This function makes a matrix transformation of docs in the topic space.\n",
    "    '''\n",
    "    top_dist =[]\n",
    "    keys = []\n",
    "\n",
    "    for d in corpus:\n",
    "        tmp = {i:0 for i in range(num_topics)}\n",
    "        tmp.update(dict(model[d]))\n",
    "        vals = list(OrderedDict(tmp).values())\n",
    "        top_dist += [array(vals)]\n",
    "        if kwords:\n",
    "            keys += [array(vals).argmax()]\n",
    "\n",
    "    return array(top_dist), keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:43.989720Z",
     "start_time": "2020-04-14T13:50:39.648341Z"
    }
   },
   "outputs": [],
   "source": [
    "top_dist1, _ = get_doc_topic_dist(model, lda_corpus1)\n",
    "top_dist2, _ = get_doc_topic_dist(model, lda_corpus2)\n",
    "\n",
    "print(\"Intra similarity: cosine similarity for corresponding parts of a doc(higher is better):\")\n",
    "print(mean([cosine_similarity(c1.reshape(1, -1), c2.reshape(1, -1))[0][0] for c1,c2 in zip(top_dist1, top_dist2)]))\n",
    "\n",
    "random_pairs = np.random.randint(0, len(df['Q3']), size=(400, 2))\n",
    "\n",
    "print(\"Inter similarity: cosine similarity between random parts (lower is better):\")\n",
    "print(np.mean([cosine_similarity(top_dist1[i[0]].reshape(1, -1), top_dist2[i[1]].reshape(1, -1)) for i in random_pairs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on these results, we could revisit and adjust the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Dominant topic and its percentage contribution in each document?\n",
    "In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.\n",
    "\n",
    "This way, you will know which document belongs predominantly to which topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:44.007526Z",
     "start_time": "2020-04-14T13:50:43.993175Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=dictionary):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:51.459882Z",
     "start_time": "2020-04-14T13:50:44.010481Z"
    }
   },
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=dictionary)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most representative sentence for each topic\n",
    "Sometimes you want to get samples of sentences that most represent a given topic. This code gets the most exemplar sentence for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:51.483327Z",
     "start_time": "2020-04-14T13:50:51.461700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display setting to show more characters in column\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution of Word Counts in Documents\n",
    "When working with a large number of documents, you want to know how big the documents are as a whole and by topic. Let’s plot the document word counts distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:50:51.489121Z",
     "start_time": "2020-04-14T13:50:51.484950Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_lens = [len(str(d)) for d in df_dominant_topic.Text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:00.387929Z",
     "start_time": "2020-04-14T13:50:51.495457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(16,7), dpi=160)\n",
    "plt.hist(doc_lens, color='navy')\n",
    "plt.text(750, 100, \"Mean   : \" + str(round(np.mean(doc_lens))))\n",
    "plt.text(750,  90, \"Median : \" + str(round(np.median(doc_lens))))\n",
    "plt.text(750,  80, \"Stdev   : \" + str(round(np.std(doc_lens))))\n",
    "plt.text(750,  70, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\n",
    "plt.text(750,  60, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n",
    "\n",
    "plt.gca().set(ylabel='Number of Documents', xlabel='Document Word Count')\n",
    "plt.tick_params(size=16)\n",
    "#plt.xticks(np.linspace(0,1000,9))\n",
    "plt.title('Distribution of Document Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using seaborn for the different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:02.762031Z",
     "start_time": "2020-04-14T13:51:00.389954Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(2,2,figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):    \n",
    "    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n",
    "    doc_lens = [len(str(d)) for d in df_dominant_topic_sub.Text]\n",
    "    ax.hist(doc_lens, color=cols[i], bins = 20)\n",
    "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n",
    "    ax.set(xlabel='Document Word Count')\n",
    "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,1000,9))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common terms by topic\n",
    "These don't seem to marry up with the topic labels from the interactive viz which is unhelpful...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:02.769643Z",
     "start_time": "2020-04-14T13:51:02.764303Z"
    }
   },
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, a topic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:02.787250Z",
     "start_time": "2020-04-14T13:51:02.772334Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_summaries = []\n",
    "print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "# start at 1\n",
    "for i in range(0, num_topics):\n",
    "    print('Topic '+str(i)+' |---------------------\\n')\n",
    "    tmp = explore_topic(model,topic_number=i, topn=10, output=True )\n",
    "#     print tmp[:5]\n",
    "    topic_summaries += [tmp[:5]]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, it's possible to inspect each topic and assign a human-interpretable label to it. Here I labeled them as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds of Top N Keywords in Each Topic\n",
    "Though you’ve already seen what are the topic keywords in each topic, a word cloud with the size of the words proportional to the weight is a pleasant sight. The coloring of the topics I’ve taken here is followed in the subsequent plots as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:09.021477Z",
     "start_time": "2020-04-14T13:51:02.793603Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stopwords,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating human readable labels for topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:09.026550Z",
     "start_time": "2020-04-14T13:51:09.023335Z"
    }
   },
   "outputs": [],
   "source": [
    "top_labels = {0: 'High risk with underlying health issue',\n",
    "              1:'Shopping and online shopping problems',\n",
    "              2:'Home delivery slot access',\n",
    "              3:'Vulnerable but not extremely, help',\n",
    "             10: 'NA'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:09.038213Z",
     "start_time": "2020-04-14T13:51:09.028814Z"
    }
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def paper_to_wordlist( paper, remove_stopwords=True ):\n",
    "    '''\n",
    "        Function converts text to a sequence of words,\n",
    "        Returns a list of words.\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # 1. Remove non-letters\n",
    "    paper_text = re.sub(\"[^a-zA-Z]\",\" \", paper)\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = paper_text.lower().split()\n",
    "    # 3. Remove stop words\n",
    "    words = [w for w in words if not w in stops]\n",
    "    # 4. Remove short words\n",
    "    words = [t for t in words if len(t) > 2]\n",
    "    # 5. lemmatizing\n",
    "    words = [nltk.stem.WordNetLemmatizer().lemmatize(t) for t in words]\n",
    "\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:09.402422Z",
     "start_time": "2020-04-14T13:51:09.039919Z"
    }
   },
   "outputs": [],
   "source": [
    "tvectorizer = TfidfVectorizer(input='content', analyzer = 'word', lowercase=True, stop_words='english',\\\n",
    "                                  tokenizer=paper_to_wordlist, ngram_range=(1, 3), min_df=40, max_df=0.20,\\\n",
    "                                  norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "\n",
    "dtm = tvectorizer.fit_transform(df['Q3']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:10.583095Z",
     "start_time": "2020-04-14T13:51:09.404219Z"
    }
   },
   "outputs": [],
   "source": [
    "top_dist =[]\n",
    "for d in corpus:\n",
    "    tmp = {i:0 for i in range(num_topics)}\n",
    "    tmp.update(dict(model[d]))\n",
    "    vals = list(OrderedDict(tmp).values())\n",
    "    top_dist += [array(vals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:11.775451Z",
     "start_time": "2020-04-14T13:51:10.585077Z"
    }
   },
   "outputs": [],
   "source": [
    "top_dist, lda_keys= get_doc_topic_dist(model, corpus, True)\n",
    "features = tvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:11.808625Z",
     "start_time": "2020-04-14T13:51:11.777383Z"
    }
   },
   "outputs": [],
   "source": [
    "top_ws = []\n",
    "for n in range(len(dtm)):\n",
    "    inds = int0(argsort(dtm[n])[::-1][:4])\n",
    "    tmp = [features[i] for i in inds]\n",
    "    \n",
    "    top_ws += [' '.join(tmp)]\n",
    "    \n",
    "df['Text_Rep'] = pd.DataFrame(top_ws)\n",
    "# replace NA with cluster 10.0\n",
    "df['clusters'] = pd.DataFrame(lda_keys)\n",
    "df['clusters'].fillna(10, inplace=True)\n",
    "# used as key later, need int\n",
    "df['clusters'] = df['clusters'].astype(int)\n",
    "\n",
    "cluster_colors = {0: 'blue', 1: 'green', 2: 'yellow', 3: 'red', 4: 'skyblue', 5:'salmon', 6:'orange', 7:'maroon', 8:'crimson', 9:'black', 10:'gray'}\n",
    "\n",
    "df['colors'] = df['clusters'].apply(lambda l: cluster_colors[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:11.889133Z",
     "start_time": "2020-04-14T13:51:11.810589Z"
    }
   },
   "outputs": [],
   "source": [
    "#?TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:20.061567Z",
     "start_time": "2020-04-14T13:51:11.891139Z"
    }
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(top_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:20.070888Z",
     "start_time": "2020-04-14T13:51:20.063873Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['X_tsne'] =X_tsne[:, 0]\n",
    "df['Y_tsne'] =X_tsne[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:20.088336Z",
     "start_time": "2020-04-14T13:51:20.073570Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:20.098703Z",
     "start_time": "2020-04-14T13:51:20.090585Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:20.132433Z",
     "start_time": "2020-04-14T13:51:20.100887Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "source = ColumnDataSource(dict(\n",
    "    x=df['X_tsne'],\n",
    "    y=df['Y_tsne'],\n",
    "    color=df['colors'],\n",
    "    #label = df['Q3'].progress_map(top_labels),\n",
    "    label=df['clusters'].apply(lambda l: top_labels[l]),\n",
    "#     msize= df['marker_size'],\n",
    "    topic_key= df['clusters'],\n",
    "    title= df[u'Q3'],\n",
    "    content = df['Text_Rep']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:51:20.221570Z",
     "start_time": "2020-04-14T13:51:20.134686Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "title = 'T-SNE visualization of topics'\n",
    "\n",
    "plot_lda = figure(plot_width=1000, plot_height=600,\n",
    "                     title=title, tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n",
    "                     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x='x', y='y', legend='label', source=source,\n",
    "                 color='color', alpha=0.8, size=10)#'msize', )\n",
    "\n",
    "#plot_lda.scatter(x='x', y='y', legend='label', source=source,\n",
    "#                 color='color', alpha=0.8, size=10)#'msize', )\n",
    "\n",
    "# hover tools\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips = {\"content\": \"Title: @title, KeyWords: @content - Topic: @topic_key \"}\n",
    "plot_lda.legend.location = \"top_left\"\n",
    "\n",
    "show(plot_lda)\n",
    "\n",
    "#save the plot\n",
    "# save(plot_lda, '{}.html'.format(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alterantive tsne approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:02:00.822395Z",
     "start_time": "2020-04-14T14:02:00.817942Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:57:08.515166Z",
     "start_time": "2020-04-14T13:57:08.492797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:07:46.627333Z",
     "start_time": "2020-04-14T14:07:46.601515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html\n",
    "* https://www.kaggle.com/ykhorramz/lda-and-t-sne-interactive-visualization\n",
    "* https://tech.goibibo.com/key-topics-extraction-and-contextual-sentiment-of-users-reviews-20e63c0fd7ca  \n",
    "* https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
