{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wordcloud\n",
    "# !pip3 install polyglot\n",
    "# !pip3 install pyicu\n",
    "# !pip3 install pycld2\n",
    "# !pip3 install morfessor\n",
    "# !pip3 install polyglot\n",
    "# !pip3 install fuzzywuzzy\n",
    "# !pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np \n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, RegexpParser, tree\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "import operator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import string \n",
    "\n",
    "## https://markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/\n",
    "from polyglot.detect import Detector\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "\n",
    "survey_filename = os.path.join(DATA_DIR, \"uis_20200401_20200409.csv\")\n",
    "df = pd.read_csv(survey_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some row duplication present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"rows: {df.shape[0]}\\nunique clientIds: {df.intents_clientID.nunique()}\")\n",
    "print(f\"unique primary key: {df.primary_key.nunique()}\\nunique session_ids: {df.session_id.nunique()}\\n\")\n",
    "# print(df.columns)\n",
    "print(df[df.session_id.isna()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the closer these numbers are to # unique primary_key, the better\n",
    "df.Q3_y.nunique(), df.Q3_x.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"primary_key\", inplace = True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for sentence tokenization, part of speech tagging, PII placeholder stripping, ngram computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pii_filtered = [\"DATE_OF_BIRTH\", \"EMAIL_ADDRESS\", \"PASSPORT\", \"PERSON_NAME\", \n",
    "                \"PHONE_NUMBER\", \"STREET_ADDRESS\", \"UK_NATIONAL_INSURANCE_NUMBER\", \"UK_PASSPORT\"]\n",
    "pii_regex = \"|\".join([f\"\\\\[{p}\\\\]\" for p in pii_filtered])\n",
    "pii_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation) + ['’']\n",
    "token_blacklist = stop_words + punctuation + pii_filtered\n",
    "\n",
    "def split_sentences(comment):\n",
    "    return nltk.sent_tokenize(comment)\n",
    "\n",
    "def remove_stopwords_punctation(sentences):\n",
    "    return [[(t[0], t[1], t[2]) for t in sent if t[0].lower() not in token_blacklist] for sent in sentences]\n",
    "\n",
    "def replace_pii_regex(text):\n",
    "    return re.sub(pii_regex, \"\", text)\n",
    "\n",
    "def part_of_speech_tag(comment):\n",
    "    sentences = split_sentences(comment)\n",
    "    return [[(token.text, token.tag_, token.lemma_) for token in nlp(sentence)] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"This is a test with punctuation’. this is another sentence.\"\n",
    "processed_t = part_of_speech_tag(t)\n",
    "processed_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect feedback language\n",
    "There is a bit of foreign language spam in some responses, detect non (primarily) english comments and drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    if text!=\"-\":\n",
    "        try:\n",
    "            langs = {language.confidence:language.code for language in Detector(text, quiet=True).languages}\n",
    "            return langs[max(langs.keys())]\n",
    "        except:\n",
    "            return f\"[ERROR] {text}\"\n",
    "    return \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Q3_pii_removed'] = df['Q3_x'].progress_map(replace_pii_regex)\n",
    "df = df[(df.Q3_pii_removed.str.len()<4000)]\n",
    "df['language'] = df['Q3_pii_removed'].progress_map(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dist = df['language'].value_counts().to_dict()\n",
    "print(f\"Number of unique languages: {len(lang_dist)}\")\n",
    "print(f\"English: {lang_dist['en']/sum(lang_dist.values()):.2%}\")\n",
    "print(f\"-: {lang_dist['-']/sum(lang_dist.values()):.2%}\")\n",
    "list(lang_dist.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_en'] = df['language'].isin([\"en\", \"un\", \"-\", \"sco\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['is_en']]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tag\n",
    "Run this the first time and save, then just load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tag'] = df[['Q3_pii_removed', 'is_en']].progress_apply(lambda x: part_of_speech_tag(x[0]) \n",
    "                                                     if x[1] else [], axis=1)\n",
    "df['lemmas'] = df['pos_tag'].progress_map(lambda x: [token[2] for sent in x for token in sent])\n",
    "\n",
    "df['words'] = df['pos_tag'].progress_map(lambda x: [token[0] for sent in x for token in sent])\n",
    "\n",
    "df.to_csv(os.path.join(DATA_DIR, \"uis_20200401_20200409_lang_pos.csv\"), index=False)\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, \"uis_20200401_20200409_lang_pos.csv\"))\n",
    "df['pos_tag'] = df['pos_tag'].map(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract noun and verb phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_of_speech_tag(df.Q3_pii_removed.iloc[0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    cc:\n",
    "    {<CC>}\n",
    "    pronoun:\n",
    "    {<DT><IN><PRP>}\n",
    "    {<IN>?<PRP>}\n",
    "    noun_verb:\n",
    "    {<IN>?<JJ.*>*<NN.*>+<HYPH>?<VBD|VBN|VBG><NN.*>*}\n",
    "    verb:\n",
    "    {<IN|TO>*<VB.*><IN|RP|TO>?<RB|WRB|TO>*<IN|TO>*}\n",
    "    {<MD><VB.*><RB><IN>+}\n",
    "    {<WRB|WP><TO>?<VB.*>+}\n",
    "    {<TO><VB><IN>}\n",
    "    {<TO>?<VB.*><IN|RP>?<WRB|RP|WP>?<IN|TO>?<VB.*>?}\n",
    "    {<VB.*><TO><VB.*><RB>*<TO>?}\n",
    "    {<IN><EX><VB.*>}\n",
    "    {<RB><TO><VB.*>+}\n",
    "    {<TO>?<VB.*><IN|WDT|WP|RP>}\n",
    "    {<WP><VB.*>}\n",
    "    {<VB.*><RB.*>+<VB.*>*<IN|TO>?}\n",
    "    {<WDT>?<TO>?<MD|VB.*>?<RB>?<TO|IN>?<V.*>+<CC>?<V.*>*<IN|RP>?<IN>*}\n",
    "    {<MD><RB>*<VB.*>*}\n",
    "    {<VB.*><IN|TO><IN>*}\n",
    "    {<TO><VB.*><IN>*}\n",
    "    {<VB.*>}\n",
    "    prep_noun:\n",
    "    {(<CD><IN><DT>)?<IN><JJ.*>*<NN.*>}\n",
    "    {<IN><NN.*><JJ.*>?<NN.*>+}\n",
    "    {<IN><NN.*><HYPH>?<NN.*>*}\n",
    "    {<IN>+<PRP\\$>?<NN><CD>?}\n",
    "    {<IN><CD><.*>}\n",
    "    {<RB><RBS>?<CD|JJ.*>?<NN.*>+}\n",
    "    {<RP>?<IN>+<JJ.*>*<NN.*>+}\n",
    "    {<IN><DT><NN.*><JJ.*>*<NN><HYPH>?<NN>}\n",
    "    {<IN><NN.*>(<HYPH>?<NN.*>)?}\n",
    "    {<JJ.*>*<IN><DT>?<NN.*>+<CD>?<NN.*>?}\n",
    "    {<IN>+<DT>*<JJ>?<CD>?<NN.*>+<CD>?<NN.*>?}\n",
    "    noun:\n",
    "    {<CD><NN.*>}\n",
    "    {<PDT><PRP\\$><NN.*>+}\n",
    "    {<RB><DT>?<JJ.*>*<NN.*>}\n",
    "    {<DT><HYPH>?<NN.*>}\n",
    "    {<JJ.*><NN.*>*<CD>}\n",
    "    {<NN.*><CD><JJ.*>?}\n",
    "    {<JJ.*|NN.*><IN|TO><PRP>}\n",
    "    {<CD><NN.*><JJ.*>}\n",
    "    {<WRB><RB><JJ.*>*<NN.*>*}\n",
    "    {<DT><JJ.*>*<NN.*>+}\n",
    "    {<NN.*><CD>?<JJ.*>*<NN.*>*}\n",
    "    {<IN>+<CD>*<POS>*<IN>*<NN.*>}\n",
    "    {<IN><PRP\\$>?<JJ.*>*<NN.*>}\n",
    "    {<NN.*><HYPH><NN.*>}\n",
    "    {<DT>?<CD>?<JJ.*>?<CC>?<JJ.*>?<NN.*>+}\n",
    "    {<NN.*><HYPH>?<NN.*|JJ.*|VB.*>*}\n",
    "    {(<NN|NNS>|<NNP|NNPS>)<NNP|NN|NNS|NNPS>+}\n",
    "    {(<NN|NNS>+|<NNP|NNPS>+)<IN|CC>(<PRP\\$|DT><NN|NNS>+|<NNP|NNPS>+)}\n",
    "    {<JJ|RB|CD>*<NNP|NN|NNS|NNPS>+}\n",
    "    {<NNP|NN|NNS|NNPS>+}\n",
    "    {<CD><IN>?<NN.*>}\n",
    "    adjective:\n",
    "    {<RB>*<JJ.*><CD>?}\n",
    "    rb:\n",
    "    {<RB>+}\n",
    "    punct:\n",
    "    {<-RRB->|<-LRB->|<,>|<.>}\n",
    "    \"\"\"\n",
    "\n",
    "class Chunk:\n",
    "\n",
    "    def __init__(self, label, tokens, indices):\n",
    "        self.label = label\n",
    "        self.tokens = tokens\n",
    "        self.indices = indices\n",
    "        self.text = self.text()\n",
    "        self.lemma = self.lemma()\n",
    "        self.important_lemma = self.important_lemma()\n",
    "        self.important_word = self.important_word()\n",
    "\n",
    "    def text(self):\n",
    "        return \" \".join([w for w,  _ , _  in self.tokens])\n",
    "    \n",
    "    def lemma(self):\n",
    "        return \" \".join([l for _,  _ , l  in self.tokens])\n",
    "    \n",
    "    def tagable_words(self):\n",
    "        return [(w, pos) for w,  pos , _  in self.tokens if re.search(r\"(NN)|(VB)\", pos)]\n",
    "    \n",
    "    def important_word(self):\n",
    "        return \" \".join([w for w,  pos , _  in self.tokens if re.search(r\"(NN)|(VB)|(JJ)|(CD)\", pos) ])\n",
    "    \n",
    "    def important_lemma(self):\n",
    "        return \" \".join([l for _,  pos , l  in self.tokens if re.search(r\"(NN)|(VB)|(JJ)|(CD)\", pos) ])\n",
    "    \n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "def chunk_text(tagged):\n",
    "    chunks = parser.parse(tagged)\n",
    "    index = 0\n",
    "    segments = []\n",
    "    for el in chunks:\n",
    "        if type(el) == tree.Tree:\n",
    "            chunk = Chunk(el.label(), el.leaves(), list(range(index, index + len(el.leaves()))))\n",
    "            segments.append(chunk)\n",
    "            index += len(el.leaves())\n",
    "        else:\n",
    "            index += 1\n",
    "    return segments\n",
    "\n",
    "def extract_phrase(sentences, merge_inplace=False):\n",
    "    chunks = []\n",
    "    for sentence in sentences:\n",
    "        chunks.append(chunk_text(sentence))\n",
    "    if merge_inplace:\n",
    "        return [merge_adjacent_chunks(chunk) for chunk in chunks]\n",
    "    return chunks  \n",
    "\n",
    "def merge_adjacent_chunks(chunks):\n",
    "    merged = []\n",
    "    previous_label = \"\"\n",
    "    for chunk in chunks:\n",
    "        if chunk.label == previous_label and chunk.label != \"prep_noun\":\n",
    "            merged[-1] = Chunk(chunk.label, \n",
    "                               merged[-1].tokens + chunk.tokens, \n",
    "                               merged[-1].indices + chunk.indices)\n",
    "        else:\n",
    "            merged.append(chunk)\n",
    "        previous_label = chunk.label\n",
    "    return merged\n",
    "\n",
    "def compute_combinations(sentences, n):\n",
    "    return [chunks[i:i+n] for chunks in sentences for i in range(len(chunks)-(n-1))]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute linguistic pattern combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linguistic_patterns(df_series, n):\n",
    "    pattern_dictionary = {}\n",
    "\n",
    "    for vals in tqdm_notebook(df_series.values):\n",
    "        sents = extract_phrase(vals, True)\n",
    "                            \n",
    "        for combo in compute_combinations(sents, n):\n",
    "            key = tuple([c.label for c in combo])\n",
    "            counter_key =  tuple([c.text.lower() for c in combo])\n",
    "            \n",
    "            if key not in pattern_dictionary.keys():\n",
    "                pattern_dictionary[key]=Counter()\n",
    "\n",
    "            pattern_dictionary[key][counter_key]+=1\n",
    "                        \n",
    "    return pattern_dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_linguistic_patterns(df.pos_tag[0:10], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expression matches for themes of interest.\n",
    "Focusing tagging verbs and tagging second argument component of verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_for_theme(text):\n",
    "    if re.search(r\"self\\s?(-|\\s)\\s?employ\", text.lower()):\n",
    "        return \"self-employ\"\n",
    "    if re.search(r\"(deliver(y|(ies)|(ed)))|(slot)|(online shopping)\", text.lower()):\n",
    "        return \"delivery\"\n",
    "    if re.search(r\"vulnerable\", text.lower()):\n",
    "        return \"vulnerable\"\n",
    "    if re.search(r\"disab((led)|(ility))\", text.lower()):\n",
    "        return \"disabled\"\n",
    "    if re.search(r\"no symptom\", text.lower()):\n",
    "        return \"no-symptoms\"\n",
    "    if re.search(r\"((corona)?(virus))|(covid)\", text.lower()):\n",
    "        return \"covid-mention\"\n",
    "    if re.search(r\"\"\"((health)|(heart) (problem)|(issue)|(condition)|(attack)|(disease)|(failure))|( ms)|\"\"\"+\n",
    "                 \"\"\"(copd)|(asthma)|((type)\\s?[12])|(diabet)|\"\"\"+\n",
    "                 \"\"\"(cancer)|(dementia)|(stroke)|(illness)|(a type$)|(cough)|(leukaemia)\"\"\", text.lower()):\n",
    "        return \"health-problem\"\n",
    "    if re.search(r\"symptom\", text.lower()):\n",
    "        return \"symptoms\"\n",
    "    if re.search(r\"((at)?(\\s(very\\s)?high)?\\srisk)|(risk list)\", text.lower()):\n",
    "        return \"at-risk\"\n",
    "    if re.search(r\"\"\"((((a|'|’)m( (in|at)( my)?)?)|aged) (over(-|\\s))?\"\"\"+\n",
    "                 \"\"\"(([789][0-9]($|s|\\s))|(old)|(elderly)))|((over(-|\\s))?[789][0-9] y)\"\"\", text.lower()):\n",
    "        return \"elderly\"\n",
    "    if re.search(r\"(carer)|(care home)\", text.lower()):\n",
    "        return \"carer\"\n",
    "    if re.search(r\"(key\\s?(\\s|-)?\\s?worker)|(nurse($|\\s))|(essential worker)\", text.lower()):\n",
    "        return \"key-worker\"\n",
    "    if re.search(r\"can\\s?(no|'|’)?t work\", text.lower()):\n",
    "        return \"cannot-work\"\n",
    "    if re.search(r\"no ((work)|(income)|(money)|(wage)|(salar))\", text.lower()):\n",
    "        return \"no-income\"\n",
    "    if re.search(r\"(furlough)|(fired)|(80 %)\", text.lower()):\n",
    "        return \"laid-off\"\n",
    "    if re.search(r\"\"\"(((can\\s?(no|'|’)?t (get|buy|(shop for)))|\"\"\"+\n",
    "                 \"\"\"((do not)?ha(ve|d) )(no|any|(not enough))?) (food|groceries))\"\"\", \n",
    "                 text.lower()):\n",
    "        return \"cannot-get-food\"\n",
    "    if re.search(r\"can\\s?(no|'|’)?t get ((med)|(prescription))\", text.lower()):\n",
    "        return \"cannot-get-med\"\n",
    "    if re.search(r\"(^med)|(prescription)\", text.lower()):\n",
    "        return \"get-med\"\n",
    "    if re.search(r\"(travel(\\s(advi[sc]e)|(status))?)|(flight)|(destination)\", text.lower()):\n",
    "        return \"travel\"\n",
    "    if re.search(r\"\"\"(no\\s)(\\w*\\s)?((info)|(clarification)|(advi[sc]e)|((contact )?((details)|(number)))|\"\"\"+\n",
    "                 \"\"\"(answer)|(update)|(clarity)|(guid(e|(ance)))|(list)|(definition)|\"\"\"+\n",
    "                 \"\"\"(address)|(link)|(form)|(contact)|(mention))\"\"\"\n",
    "                 , text.lower()):\n",
    "        return \"no-information\"\n",
    "    if re.search(r\"\"\"(info)|(clarification)|(advi[sc]e)|((contact )?((details)|(number)))|\"\"\"+\n",
    "                 \"\"\"(answer)|(update)|(clarity)|(guid(e|(ance)))|(list)|(definition)|\"\"\"+\n",
    "                 \"\"\"(address)|(link)|(form)|(contact)\"\"\"\n",
    "                 , text.lower()):\n",
    "        return \"information\"\n",
    "    if re.search(r\"\"\"(no)\\s((letter)|(t(e)?xt)|(message)|(e(\\s|(\\s?-\\s?))?mail)|\"\"\"+\n",
    "                 \"\"\"(alert)|(notice)|(communication))\"\"\", text.lower()):\n",
    "        return \"no-correspondence\"\n",
    "    if re.search(r\"(letter)|(t(e)?xt)|(message)|(e(\\s|(\\s?-\\s?))?mail)|(alert)|(notice)\", text.lower()):\n",
    "        return \"correspondence\"\n",
    "    if re.search(r\"(no\\s?((family)|(one)))|(nothing)|(nobody)\", text.lower()):\n",
    "        return \"no-one\"\n",
    "    if re.search(r\"no ((support)|(aid)|(help)|(assistance)|(access)|(priority))\", text.lower()):\n",
    "        return \"no-support\"\n",
    "    if re.search(r\"(support)|(aid)|(help)|(assistance)|(access)|(priority)\", text.lower()):\n",
    "        return \"support\"\n",
    "    if re.search(r\"(child)|((^|\\s)son)|(daughter)\", text.lower()):\n",
    "        return \"child\"\n",
    "    if re.search(r\"\"\"(parent)|(husband)|(wife)|(partner)|\"\"\"+\n",
    "                 \"\"\"((mo|fa)ther)|(famil(y|(ies)))|(m[uo]m)|(dad)\"\"\", text.lower()):\n",
    "        return \"family\"\n",
    "    if re.search(r\"(rule)|(restriction)|(measure)|(rights)\", text.lower()):\n",
    "        return \"rules\"\n",
    "    if re.search(r\"((no)|(a(ny)?)) ((way)|(option)|(choice)|(means)|(idea))\", text.lower()):\n",
    "        return \"uncertainty\"\n",
    "    if re.search(r\"work ((for)|(in)|(at)|(on))\", text.lower()):\n",
    "        return \"work\"\n",
    "    if re.search(r\"((self\\s|-)?isolat((ion)|(e)|(ing)))|(lock\\s?(\\s|-)?\\s?down)\", text.lower()):\n",
    "        return \"self-isolation\"\n",
    "    if re.search(r\"(driv(ing|ers)\\s)?licen[sc]e\", text.lower()):\n",
    "        return \"license\"\n",
    "    if re.search(r\"passport\", text.lower()):\n",
    "        return \"passport\"\n",
    "    if re.search(r\"pension\", text.lower()):\n",
    "        return \"pension\"\n",
    "    if re.search(r\"(^|\\s)h((ome)|(ouse))\", text.lower()):\n",
    "        return \"home-mention\"\n",
    "    if re.search(r\"(employ)|(work)|(job)|(business)|(company)\", text.lower()):\n",
    "        return \"work-mention\"\n",
    "    if re.search(r\"(benefit)|(universal credit)|(eligible)|(esa)|(ssp)|(pip)|(allowance)\", text.lower()):\n",
    "        return \"benefit\"\n",
    "    if re.search(r\"(school)|(student)\", text.lower()):\n",
    "        return \"school\"\n",
    "    if re.search(r\"(food)|(supplies)|(shopping)|(groceries)\", text.lower()):\n",
    "        return \"goods\"\n",
    "    if re.search(r\"(money)|(grant)|(fund)|(relief)\", text.lower()):\n",
    "        return \"given-money\"\n",
    "    if re.search(r\"(bill)|(tax)|(mortgage)|(rent)|(loan)|(debt)|(fine)|(fee)|(insurance)\", text.lower()):\n",
    "        return \"bills-to-pay\"\n",
    "    if re.search(r\"scheme\", text.lower()):\n",
    "        return \"scheme\"\n",
    "    if re.search(r\"(^|\\s)visa($|\\s)\", text.lower()):\n",
    "        return \"visa\"\n",
    "    if re.search(r\"(data)|(cases)|(situation)|(stat(istic)?s?$)|(status)|(news)|(progress)\", text.lower()):\n",
    "        return \"data\"\n",
    "    if re.search(r\"dea((th)|d)\", text.lower()):\n",
    "        return \"death\"\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_for_theme(\"stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_group_verbs(verb):\n",
    "    if re.search(r\"\"\"(f(i|(ou))nd)|(look)|(search)|(clarify)|(ask)|(read)|([ei]nquire)|\"\"\"+\n",
    "                 \"\"\"(obtain)|(seek)|(know)|((^|\\s)see($|\\s))|(understand)\"\"\", verb):\n",
    "        return \"find-smthg\"\n",
    "    if re.search(r\"(access)|(check)|(complete)|(cancel)|(book)|(confirm)\", verb):\n",
    "        return \"access-smthg\"\n",
    "    if re.search(r\"(get)|(take)|(claim)|(receive)|(sent)|(collect)\", verb):\n",
    "        return \"acquire-smthg\"\n",
    "    if re.search(r\"(renew)|(change)|(update)|(inform$)|(notify)\", verb):\n",
    "        return \"change-smthg\"\n",
    "    if re.search(r\"(appl(y|(ied)))|(register)|(qualify)|(sign)\", verb):\n",
    "        return \"apply-smthg\"\n",
    "    if re.search(r\"pa(y|(id)|(yed))\", verb):\n",
    "        return \"pay-smthg\"\n",
    "    if re.search(r\"(contact)|(report)\", verb):\n",
    "        return \"contact-smthg\"\n",
    "    if re.search(r\"(work)|(employ)\", verb):\n",
    "        return \"work-smwhr\"\n",
    "    if re.search(r\"(need)|(want)|(require)|(request)|(would like)|(order)\", verb):\n",
    "        return \"need-smthg\"\n",
    "    if re.search(r\"(have)|((a|'|’|^)m($|\\s))|(feel($|\\s))\", verb):\n",
    "        return \"my-situation\"\n",
    "    if re.search(r\"(has)|(((a|we)|'|’|^)re($|\\s))\", verb):\n",
    "        return \"others-situation\"\n",
    "    if re.search(r\"(had)|((i|'|’|^)s($|\\s))|(was)\", verb):\n",
    "        return \"unclear-situation\"\n",
    "    if re.search(r\"travel\", verb):\n",
    "        return \"travel\"\n",
    "    if re.search(r\"(liv(e|(ing)))|(stay)\", verb):\n",
    "        return \"living\"\n",
    "    if re.search(r\"(do)|(make)\", verb):\n",
    "        return \"do-smthng\"\n",
    "    if re.search(r\"go($|\\s)\", verb):\n",
    "        return \"go-smwhr\"\n",
    "    if re.search(r\"(give)|(provide)\", verb):\n",
    "        return \"give-smthng\"\n",
    "    if re.search(r\"(help)|(protect)|(support)\", verb):\n",
    "        return \"help\"\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[7]\n",
    "example = df[df.Q3_x.str.contains(\"letter\")].iloc[0]\n",
    "print(f\"Themetatic category for entire comment: {regex_for_theme(example.Q3_x)}\")\n",
    "\n",
    "print(example.Q3_x)\n",
    "print()\n",
    "print(example.pos_tag)\n",
    "print()\n",
    "for sent in extract_phrase(example.pos_tag, False):\n",
    "    for chunk in sent:\n",
    "        theme = \"\"\n",
    "        if chunk.label in [\"verb\"]:\n",
    "            theme = regex_group_verbs(chunk.text.lower())\n",
    "        if chunk.label in [\"noun\", \"prep_noun\", \"noun_verb\"]:\n",
    "            theme = regex_for_theme(chunk.text.lower()) \n",
    "            \n",
    "        print(\"{0:10} {1:35} {2:20} {3}\".format(chunk.label.upper(), chunk.text, theme, chunk.indices))\n",
    "    print()\n",
    "    for combo in compute_combinations([sent], 2):\n",
    "        print(f\"{combo[0].text}, {combo[1].text}\")\n",
    "        \n",
    "#     for combo in compute_combinations([sent], 3):\n",
    "#         print(f\"{combo[0].text}, {combo[1].text}, {combo[2].text}\")\n",
    "    print(\"=====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a mother in law who is 90yrs old, lives on her own, has diabetes and asthma, \n",
    "so there for self isolating. I am checking on her daily, taking food etc. \n",
    "I go there once a week to change her bedding, which she is safely in her conservatory. \n",
    "I work for a train company but not in a safety critical role, \n",
    "I only serve drinks/snacks, which has been suspended now. \n",
    "They want me to come in to clean inside trains ( where the public are) \n",
    "and the stations. I have said that I don’t want to come in as caring for my mother in law. They have told me \n",
    "that I will have to go off sick and get a sick note from GP? \n",
    "Then I will only get statutory sick pay! I don’t want to run the risk \n",
    "of infecting my at risk elderly parent. Where do I stand?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect arg1-arg2 grammatical patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pos_tag[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_d = compute_linguistic_patterns(df.pos_tag, 2)\n",
    "len(pattern_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_of_interest = [('verb', 'noun'),\n",
    "('noun', 'prep_noun'),\n",
    "('prep_noun', 'prep_noun'),\n",
    "('verb', 'noun_verb'),\n",
    "('verb', 'prep_noun'),\n",
    "('noun', 'noun_verb'),\n",
    "('noun_verb', 'prep_noun')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute `arg1` - `arg2` co-occurrence db - couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_db = {}\n",
    "\n",
    "for vals in tqdm_notebook(df.pos_tag.values):\n",
    "    sents = extract_phrase(vals, True)\n",
    "    for combo in compute_combinations(sents, 2):\n",
    "        key = (combo[0].label, combo[1].label)\n",
    "        arg1 = combo[0].text.lower()\n",
    "        arg2 = combo[1].text.lower()\n",
    "#         arg2 = \" \".join([w.lower() for w,_ in combo[1].tagable_words()])\n",
    "        \n",
    "        if key not in pattern_db.keys():\n",
    "            pattern_db[key] = {}\n",
    "        if arg1 not in pattern_db[key].keys():\n",
    "            pattern_db[key][arg1] = Counter()\n",
    "            \n",
    "        pattern_db[key][arg1][arg2]+=1\n",
    "\n",
    "print(f\"There are {len(pattern_db)} possible grammatical combos.\")\n",
    "for i, (k,v) in enumerate(sorted(pattern_db.items(),\n",
    "                         key = lambda x: len(x[1].values()),\n",
    "                         reverse= True)[0:15],\n",
    "                                 1):\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of verbs currently not being captured/categorized with regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_verbs = [key.lower() for key, value in sorted(pattern_db[('verb', 'noun')].items(), \n",
    "                         key = lambda x: sum(x[1].values()), \n",
    "                         reverse= True)[0:100]]\n",
    "counter = 0\n",
    "for verb in top_100_verbs:\n",
    "    if regex_group_verbs(verb)== \"unknown\":\n",
    "        counter+=1\n",
    "        print(counter, verb)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create co-occurrence tables for `verb_theme-verb`, `verb_theme-argument_theme`, argument_theme-argument` pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_argument_theme_dictionary(dict_new, dict_old):\n",
    "    for theme, value in dict_new.items():\n",
    "        if theme not in dict_old.keys():\n",
    "            dict_old[theme] = Counter()\n",
    "        for val,count in value.items():\n",
    "            dict_old[theme][val]+=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_themes = {}\n",
    "verb_argument_themes = {}\n",
    "argument_themes = {}\n",
    "\n",
    "verb_argument_fillers = {}\n",
    "\n",
    "for pattern in [('verb', 'noun'), ('verb', 'prep_noun')]:\n",
    "    print(f\"There are {len(pattern_db[pattern])} {pattern[0]}s, accompanied by {pattern[1]}s.\")\n",
    "    for i, (arg1, arg2) in enumerate(sorted(pattern_db[pattern].items(),\n",
    "                             key = lambda x: sum(x[1].values()),\n",
    "                             reverse= True),\n",
    "                                     1):\n",
    "        verb_theme = f\"{regex_group_verbs(arg1)}\".upper()\n",
    "\n",
    "        if verb_theme not in verb_themes.keys():\n",
    "            verb_themes[verb_theme] = Counter()\n",
    "        \n",
    "        verb_themes[verb_theme][arg1] += sum(arg2.values())  \n",
    "        \n",
    "#         print(f\"{i}. {arg1} :: {sum(arg2.values())} [{verb_theme}] \\n-----------\")\n",
    "        \n",
    "        if verb_theme not in verb_argument_themes.keys():\n",
    "            verb_argument_themes[verb_theme] = {}\n",
    "\n",
    "        local_themes = {}\n",
    "        \n",
    "        for j, (arg2_val, arg2_counts) in enumerate(arg2.items(), 1):\n",
    "            theme = f\"{regex_for_theme(arg2_val)}\".upper()\n",
    "            if theme not in local_themes.keys():\n",
    "                local_themes[theme] = Counter()\n",
    "                \n",
    "            if theme not in argument_themes.keys():   \n",
    "                argument_themes[theme] = Counter()   \n",
    "                \n",
    "            local_themes[theme][arg2_val]+=arg2_counts   \n",
    "            argument_themes[theme][arg2_val]+=arg2_counts  \n",
    "            \n",
    "            verb_argument_theme = (verb_theme, theme)\n",
    "            \n",
    "            if verb_argument_theme not in verb_argument_fillers.keys():\n",
    "                verb_argument_fillers[verb_argument_theme] = {}\n",
    "                \n",
    "            if arg1 not in verb_argument_fillers[verb_argument_theme].keys():\n",
    "                verb_argument_fillers[verb_argument_theme][arg1] = Counter() \n",
    "                \n",
    "            verb_argument_fillers[verb_argument_theme][arg1][arg2_val] += arg2_counts\n",
    "            \n",
    "            \n",
    "        update_argument_theme_dictionary(local_themes, verb_argument_themes[verb_theme])\n",
    "        \n",
    "#         for l, (key,value) in enumerate(sorted(local_themes.items(),\n",
    "#                              key = lambda x: sum(x[1].values()),\n",
    "#                              reverse= True)[0:10],\n",
    "#                                      1):\n",
    "#             print(f\"{l}. {key}:: {sum(value.values())}\")\n",
    "#             for argument, count in value.most_common(5):\n",
    "#                 print(f\"{argument}: {count}\")\n",
    "#             print(\"\")\n",
    "#         print(\"=======\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(argument_themes), len(verb_themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in list(verb_argument_fillers.items())[0:10]:\n",
    "    print(key, sum([vs for v in value.values() for vs in v.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe with 20 most frequently occurring generic phrases and their components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = []\n",
    "for pattern, values in sorted(verb_argument_fillers.items(),\n",
    "                           key = lambda x: sum([vs for v in x[1].values() for vs in v.values()]),\n",
    "                           reverse = True)[0:100]:\n",
    "    if 'UNKNOWN' not in pattern:\n",
    "        total_occ = sum([vs for v in values.values() for vs in v.values()])\n",
    "        print(pattern, total_occ)\n",
    "        \n",
    "        local_arguments = Counter()\n",
    "        local_verbs = Counter()\n",
    "        \n",
    "        for verb, argument_values in sorted(verb_argument_fillers[pattern].items(),\n",
    "                                   key = lambda x: sum(x[1].values()),\n",
    "                                   reverse = True):\n",
    "#             print(verb, sum(argument_values.values()))\n",
    "            local_verbs[verb] += sum(argument_values.values())\n",
    "#             print(argument_values.most_common(5))\n",
    "            for argument, count in argument_values.most_common():\n",
    "                local_arguments[argument] += count\n",
    "        \n",
    "        verbs = [f\"{k} : {v}\" for k,v in local_verbs.most_common()]\n",
    "        arguments = [f\"{k} : {v}\" for k,v in local_arguments.most_common()]\n",
    "        \n",
    "        print(\"verbs:\\n\", verbs[0:10])\n",
    "        print(\"arguments:\\n\", arguments[0:10])\n",
    "        \n",
    "        row_list.append({\"Generic phrase\": f\"{pattern[0]} - {pattern[1]}\",\n",
    "                         \"# of times occured\": total_occ,\n",
    "                        \"Verb category\": f\"{pattern[0]}\",\n",
    "                         \"Total # of verbs\" : sum(local_verbs.values()),\n",
    "                         \"# of unique verbs\" : len(local_verbs),\n",
    "                        \"Verb values\": \"\\n\".join(verbs[0:10]),\n",
    "                        \"Argument category\": f\"{pattern[1]}\",\n",
    "                        \"Total # of arguments\" : sum(local_arguments.values()),\n",
    "                         \"# of unique arguments\" : len(local_arguments),\n",
    "                        \"Argument values\": \"\\n\".join(arguments[0:10])})\n",
    "        print(\"====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(row_list)[0:20].to_csv(os.path.join(DATA_DIR, \"generic_phrase_counts_top_20.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview themes + values assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (key,value) in enumerate(sorted([(k,v) for k,v in verb_argument_themes.items() if k != \"UNKNOWN\"],\n",
    "                                       key = lambda x: sum([sum(counter.values()) for counter in x[1].values()]),\n",
    "                                      reverse=True),1):\n",
    "\n",
    "    print(f\"{i}. {key} {sum([sum(counter.values()) for counter in value.values()])} {len(value)} \\n======\")\n",
    "    for j, (argument, counter) in enumerate(sorted([(k,v) for k,v in value.items() if k != \"UNKNOWN\"],\n",
    "                                                   key = lambda x: sum(x[1].values()),\n",
    "                                                   reverse=True\n",
    "                                                  )\n",
    "                                            , 1):\n",
    "\n",
    "        print(f\"{argument} {sum(counter.values())}\")\n",
    "        for l, (arg_theme, vals) in enumerate(counter.most_common(5)):\n",
    "            print(f\"{arg_theme}: {vals}\")\n",
    "        print(\"---\")\n",
    "    print()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign themes to actions and things people are talking about \n",
    "### Tag response comments (Q3) with appropriate themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_mentions = []\n",
    "for vals in tqdm_notebook(df.pos_tag.values):\n",
    "    sents = extract_phrase(vals, True)\n",
    "    phrase_mentions.append([])\n",
    "    for combo in compute_combinations(sents, 2):\n",
    "        key = (combo[0].label, combo[1].label)\n",
    "        arg1 = combo[0].text.lower()\n",
    "        arg2 = combo[1].text.lower()\n",
    "        \n",
    "        if key in [('verb', 'noun'), ('verb', 'prep_noun'), \n",
    "                   ('verb', 'noun_verb'), ('noun','prep_noun'),\n",
    "                  ('prep_noun','noun'), ('prep_noun','prep_noun')]:\n",
    "            mention_theme = f\"{regex_group_verbs(arg1)} - {regex_for_theme(arg2)}\"\n",
    "            \n",
    "            arg1 = re.sub(r\"\\(|\\)|\\[|\\]|\\+\", \"\", arg1)\n",
    "            arg2 = re.sub(r\"\\(|\\)|\\[|\\]|\\+\", \"\", arg2)\n",
    "            phrase = f\"{arg1} {arg2}\"\n",
    "            phrase_mentions[-1].append((key, phrase, mention_theme, (arg1,arg2)))\n",
    "            \n",
    "df['theme_mentions'] = phrase_mentions       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['theme_mentions'].str.len()>0].iloc[100].theme_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['theme_mentions_list'] = df['theme_mentions'].map(lambda x: [mention for key,_,mention,_ in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_group(arg1, arg2):\n",
    "    if re.search(r\"((('|’|^(a)?)m)|(have been)|(feel))$\", arg1):\n",
    "        return re.sub(r\"^((the)|a)\\s\",\"\", arg2)\n",
    "    return \"\"\n",
    "\n",
    "def resolve_function(x):\n",
    "    res = [get_user_group(*args) for theme,_,_,args in x if \"verb\" in theme[0]]\n",
    "    return [r for r in res if r != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [[('verb', 'noun'),None,None,('feel', 'the key-worker')]]\n",
    "resolve_function(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['theme_mentions_user'] = df['theme_mentions'].map(resolve_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_groups = Counter()\n",
    "for vals in df[df['theme_mentions_user'].str.len()>0].theme_mentions_user.values:\n",
    "    for val in vals:\n",
    "        user_groups[val] +=1\n",
    "user_groups.most_common(10), \"housebound\" in user_groups, len(user_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "from difflib import SequenceMatcher as SM\n",
    "from nltk.util import ngrams\n",
    "import codecs\n",
    "\n",
    "needle = \"told register as a vulnerable person\"\n",
    "hay    = \"told to register as a vulnerable person for delivery service for on line shopping\"\n",
    "\n",
    "def find_needle(needle, hay):\n",
    "    needle_length  = len(needle.split())\n",
    "    max_sim_val    = 0\n",
    "    max_sim_string = u\"\"\n",
    "#     print(needle)\n",
    "    for ngram in ngrams(hay.split(), needle_length + int(.65*needle_length)):\n",
    "        hay_ngram = u\" \".join(ngram)\n",
    "        similarity = SM(None, hay_ngram, needle).ratio() \n",
    "        if similarity > max_sim_val:\n",
    "            max_sim_val = similarity\n",
    "            max_sim_string = hay_ngram\n",
    "    \n",
    "    if max_sim_string == \"\":\n",
    "        max_sim_string = hay\n",
    "\n",
    "    tokens = needle.split(\" \")\n",
    "    if len(tokens) == 1:\n",
    "        expression = tokens[0]\n",
    "    else:\n",
    "        expression = f\"({tokens[0]}).*({tokens[-1]})\"\n",
    "    result = regex.search(expression, max_sim_string)\n",
    "    \n",
    "    if result is not None:\n",
    "        pattern = result.group()\n",
    "        \n",
    "        return {needle: pattern}\n",
    "    return {needle:None}\n",
    "\n",
    "print(find_needle(needle, hay))\n",
    "print(find_needle(\"housebound\", \"i am housebound\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['theme_mentions', \"Q3_pii_removed\"]].iloc[142].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove special characters\n",
    "df[\"Q3_pii_removed\"] = df[\"Q3_pii_removed\"].replace(np.nan, '', regex=True)\n",
    "df[\"Q3_pii_removed\"] = df[\"Q3_pii_removed\"].progress_map(lambda x: ' '.join(\n",
    "                                    re.sub(r\"\\(|\\)|\\[|\\]|\\+\", \"\", x).split()))\n",
    "\n",
    "df[\"Q3_x_edit\"] = df[\"Q3_x\"].replace(np.nan, '', regex=True)\n",
    "df[\"Q3_x_edit\"] = df[\"Q3_x_edit\"].progress_map(lambda x: ' '.join(re.sub(r\"\\(|\\)|\\[|\\]|\\+\", \"\", x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns for `phrases` and `user_groups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['theme_mentions', \"Q3_x_edit\"]].iloc[6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['phrases_dict'] = df[['theme_mentions', \"Q3_x_edit\"]][:].\\\n",
    "            progress_apply(lambda x: [find_needle(phrase, x[1].lower()) for _,phrase,_,_  in x[0]], axis=1)\n",
    "df['phrases_list'] = df['phrases_dict'].progress_map(lambda x: [value for phrase_dict in x \n",
    "                                                                 for value in phrase_dict.values() \n",
    "                                                                 if value is not None]\n",
    "                                                if not isinstance(x, float) else [])\n",
    "df['phrases'] = df['phrases_list'].progress_map(lambda x: \", \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_counts = Counter()\n",
    "for phrase_list in df.phrases_list.values:\n",
    "    for phrase in phrase_list:\n",
    "        phrase_counts[phrase]+=1\n",
    "phrase_counts.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['phrases']!=''][['phrases', 'Q3_x_edit']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_phrases_dict'] = df[['theme_mentions_user', \"Q3_x_edit\"]][:].\\\n",
    "            progress_apply(lambda x: [find_needle(phrase, x[1].lower()) for phrase  in x[0]], axis=1)\n",
    "df['user_phrases_list'] = df['user_phrases_dict'].progress_map(lambda x: [value for phrase_dict in x \n",
    "                                                                 for value in phrase_dict.values() \n",
    "                                                                 if value is not None]\n",
    "                                                if not isinstance(x, float) else [])\n",
    "\n",
    "df['user_phrases'] = df['user_phrases_list'].progress_map(lambda x: \", \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_groups = Counter()\n",
    "for vals in df[df['user_phrases_list'].str.len()>0].user_phrases_list.values:\n",
    "    for val in vals:\n",
    "        user_groups[val] +=1\n",
    "user_groups.most_common(10), \"housebound\" in user_groups, len(user_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect missing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = 0\n",
    "for phrase_list, comment in df[~df['phrases_dict'].isna()][['phrases_dict', 'Q3_x_edit']].values:\n",
    "    for phrase_dict in phrase_list:\n",
    "        for key,value in phrase_dict.items():\n",
    "            if str(value) not in comment.lower():\n",
    "                missing+=1\n",
    "missing       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results for tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df[['primary_key', 'intents_clientID', 'visitId', 'fullVisitorId',\n",
    "       'hits_pagePath', 'Started', 'Ended', 'Q1_x', 'Q2_x', 'Q3_x_edit', 'Q4_x',\n",
    "       'Q5_x', 'Q6_x', 'Q7_x', 'Q8_x', 'session_id', 'dayofweek', 'isWeekend',\n",
    "       'hour', 'country', 'country_grouping', 'UK_region', 'UK_metro_area',\n",
    "       'channelGrouping', 'deviceCategory',\n",
    "       'total_seconds_in_session_across_days',\n",
    "       'total_pageviews_in_session_across_days', 'finding_count',\n",
    "       'updates_and_alerts_count', 'news_count', 'decisions_count',\n",
    "       'speeches_and_statements_count', 'transactions_count',\n",
    "       'regulation_count', 'guidance_count', 'business_support_count',\n",
    "       'policy_count', 'consultations_count', 'research_count',\n",
    "       'statistics_count', 'transparency_data_count',\n",
    "       'freedom_of_information_releases_count', 'incidents_count',\n",
    "       'done_page_flag', 'count_client_error', 'count_server_error',\n",
    "       'ga_visit_start_timestamp', 'ga_visit_end_timestamp',\n",
    "       'intents_started_date', 'events_sequence', 'search_terms_sequence',\n",
    "       'cleaned_search_terms_sequence', 'top_level_taxons_sequence',\n",
    "       'page_format_sequence', 'Sequence', 'PageSequence', 'flag_for_criteria',\n",
    "       'full_url_in_session_flag', 'UserID', 'UserNo', 'Name', 'Email',\n",
    "       'IP Address', 'Unique ID', 'Tracking Link', 'clientID', 'Page Path',\n",
    "       'Q1_y', 'Q2_y', 'Q3_y', 'Q4_y', 'Q5_y', 'Q6_y', 'Q7_y', 'Q8_y',\n",
    "       'Started_Date', 'Ended_Date', 'Started_Date_sub_12h', 'phrases', 'user_phrases']]\n",
    "\n",
    "df_sub.rename(columns={'Q3_x_edit':'Q3_x'}, inplace=True)\n",
    "df_sub.to_csv(os.path.join(DATA_DIR, 'uis_20200401_20200409_phrases_user_groups.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
