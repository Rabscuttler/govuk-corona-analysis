{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wordcloud\n",
    "# !pip3 install polyglot\n",
    "# !pip3 install pyicu\n",
    "# !pip3 install pycld2\n",
    "# !pip3 install morfessor\n",
    "# !pip3 install polyglot\n",
    "# !pip3 install fuzzywuzzy\n",
    "# !pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np \n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, RegexpParser, tree\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "import operator\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import string \n",
    "\n",
    "## https://markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/\n",
    "from polyglot.detect import Detector\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "\n",
    "survey_filename = os.path.join(DATA_DIR, \"joined_uis_all_of_march.csv\")\n",
    "df = pd.read_csv(survey_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some row duplication present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape, df.intents_clientID.nunique(), df.primary_key.nunique(), df.session_id.nunique())\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df[df.session_id.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"primary_key\", inplace = True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for sentence tokenization, part of speech tagging, PII placeholder stripping, ngram computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pii_filtered = [\"DATE_OF_BIRTH\", \"EMAIL_ADDRESS\", \"PASSPORT\", \"PERSON_NAME\", \n",
    "                \"PHONE_NUMBER\", \"STREET_ADDRESS\", \"UK_NATIONAL_INSURANCE_NUMBER\", \"UK_PASSPORT\"]\n",
    "pii_regex = \"|\".join([f\"\\\\[{p}\\\\]\" for p in pii_filtered])\n",
    "pii_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation) + ['’']\n",
    "token_blacklist = stop_words + punctuation + pii_filtered\n",
    "\n",
    "def split_sentences(comment):\n",
    "    return nltk.sent_tokenize(comment)\n",
    "\n",
    "def remove_stopwords_punctation(sentences):\n",
    "    return [[(t[0], t[1], t[2]) for t in sent if t[0].lower() not in token_blacklist] for sent in sentences]\n",
    "\n",
    "def replace_pii_regex(text):\n",
    "    return re.sub(pii_regex, \"\", text)\n",
    "\n",
    "def compute_ngrams(processed_comment, n, stemming=False, filtering=False):\n",
    "    # processed_comment = part_of_speech_tag(comment)\n",
    "    if filtering:\n",
    "        processed_comment = remove_stopwords_punctation(processed_comment)\n",
    "    index = 2 if stemming else 0\n",
    "    tokens = [token[index] for sent in processed_comment for token in sent]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram).lower() for ngram in ngrams]\n",
    "\n",
    "\n",
    "def part_of_speech_tag(comment):\n",
    "    sentences = split_sentences(comment)\n",
    "    return [[(token.text, token.tag_, token.lemma_) for token in nlp(sentence)] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"This is a test with punctuation’. this is another sentence.\"\n",
    "processed_t = part_of_speech_tag(t)\n",
    "compute_ngrams(processed_t, 2, stemming=False, filtering=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect feedback language\n",
    "There is a bit of foreign language spam in some responses, detect non (primarily) english comments and drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    if text!=\"-\":\n",
    "        try:\n",
    "            langs = {language.confidence:language.code for language in Detector(text, quiet=True).languages}\n",
    "            return langs[max(langs.keys())]\n",
    "        except:\n",
    "            return f\"[ERROR] {text}\"\n",
    "    return \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Q3_pii_removed'] = df['Q3'].progress_map(replace_pii_regex)\n",
    "df = df[(df.Q3_pii_removed.str.len()<4000)]\n",
    "df['language'] = df['Q3_pii_removed'].progress_map(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dist = df['language'].value_counts().to_dict()\n",
    "print(f\"Number of unique languages: {len(lang_dist)}\")\n",
    "print(f\"English: {round((lang_dist['en']*100)/sum(lang_dist.values()), 2)}%\")\n",
    "print(f\"-: {round((lang_dist['-']*100)/sum(lang_dist.values()), 2)}%\")\n",
    "list(lang_dist.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_en'] = df['language'].isin([\"en\", \"un\", \"-\", \"sco\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tag\n",
    "Run this the first time and save, then just load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['pos_tag'] = df[['Q3_pii_removed', 'is_en']].progress_apply(lambda x: part_of_speech_tag(x[0]) \n",
    "#                                                      if x[1] else [], axis=1)\n",
    "# df['lemmas'] = df['pos_tag'].progress_map(lambda x: [token[2] for sent in x for token in sent])\n",
    "\n",
    "# df['words'] = df['pos_tag'].progress_map(lambda x: [token[0] for sent in x for token in sent])\n",
    "\n",
    "# df.to_csv(os.path.join(DATA_DIR, \"joined_uis_all_of_march_lang_pos.csv\"), index=False)\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, \"joined_uis_all_of_march_lang_pos.csv\"))\n",
    "df['pos_tag'] = df['pos_tag'].map(literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract noun and verb phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_of_speech_tag(df.Q3_pii_removed.iloc[0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    cc:\n",
    "    {<CC>}\n",
    "    pronoun:\n",
    "    {<DT><IN><PRP>}\n",
    "    {<IN>?<PRP>}\n",
    "    noun_verb:\n",
    "    {<IN>?<JJ.*>*<NN.*>+<HYPH>?<VBD|VBN|VBG><NN.*>*}\n",
    "    verb:\n",
    "    {<IN>*<VB.*><IN>}\n",
    "    {<WRB><TO><VB.*>}\n",
    "    {<TO><VB.*><IN|RP>?<WRB>}\n",
    "    {<VB.*><TO><VB.*><RB>?<TO>?}\n",
    "    {<IN><EX><VB.*>}\n",
    "    {<RB><TO><VB.*>+}\n",
    "    {<TO>?<VB.*><IN|WDT|WP>}\n",
    "    {<WP><VB.*>}\n",
    "    {<VB.*><RB><VB.*>*}\n",
    "    {<WDT>?<TO>?<MD|VB.*>?<RB>?<TO|IN>?<V.*>+<CC>?<V.*>*<IN|RP>?<IN>?}\n",
    "    {<MD><RB>*<VB.*>*}\n",
    "    {<VB.*><IN|TO><IN>?}\n",
    "    {<TO><VB.*><IN>+}\n",
    "    prep_noun:\n",
    "    {<IN><NN.*><HYPH>?<NN.*>*}\n",
    "    {<IN>+<PRP\\$>?<NN><CD>?}\n",
    "    {<IN><CD><.*>}\n",
    "    {<RP>?<IN>+<JJ.*>*<NN.*>+}\n",
    "    {<IN><DT><NN.*><JJ.*>*<NN><HYPH>?<NN>}\n",
    "    {<IN><NN.*>(<HYPH>?<NN.*>)?}\n",
    "    {<JJ.*>*<IN><DT>?<NN.*>+<CD>?<NN.*>?}\n",
    "    {<IN>+<DT>*<JJ>?<CD>?<NN.*>+<CD>?<NN.*>?}\n",
    "    noun:\n",
    "    {<CD><NN.*>}\n",
    "    {<DT><NN.*>}\n",
    "    {<JJ.*><NN.*>*<CD>}\n",
    "    {<NN.*><CD><JJ.*>?}\n",
    "    {<JJ.*|NN.*><IN|TO><PRP>}\n",
    "    {<CD><NN.*><JJ.*>}\n",
    "    {<WRB><RB><JJ.*>*<NN.*>*}\n",
    "    {<DT><JJ.*>*<NN.*>+}\n",
    "    {<NN.*><CD>?<JJ.*>*<NN.*>*}\n",
    "    {<IN>+<CD>*<POS>*<IN>*<NN.*>}\n",
    "    {<IN><PRP\\$>?<JJ.*>*<NN.*>}\n",
    "    {<NN.*><HYPH><NN.*>}\n",
    "    {<DT>?<CD>?<JJ.*>?<CC>?<JJ.*>?<NN.*>+}\n",
    "    {<NN.*><HYPH>?<NN.*|JJ.*|VB.*>*}\n",
    "    {(<NN|NNS>|<NNP|NNPS>)<NNP|NN|NNS|NNPS>+}\n",
    "    {(<NN|NNS>+|<NNP|NNPS>+)<IN|CC>(<PRP\\$|DT><NN|NNS>+|<NNP|NNPS>+)}\n",
    "    {<JJ|RB|CD>*<NNP|NN|NNS|NNPS>+}\n",
    "    {<NNP|NN|NNS|NNPS>+}\n",
    "    adjective:\n",
    "    {<RB>*<JJ.*><CD>?}\n",
    "    \"\"\"\n",
    "\n",
    "class Chunk:\n",
    "\n",
    "    def __init__(self, label, tokens, indices):\n",
    "        self.label = label\n",
    "        self.tokens = tokens\n",
    "        self.indices = indices\n",
    "        self.text = self.text()\n",
    "        self.lemma = self.lemma()\n",
    "        self.important_lemma = self.important_lemma()\n",
    "        self.important_word = self.important_word()\n",
    "\n",
    "    def text(self):\n",
    "        return \" \".join([w for w,  _ , _  in self.tokens])\n",
    "    \n",
    "    def lemma(self):\n",
    "        return \" \".join([l for _,  _ , l  in self.tokens])\n",
    "    \n",
    "    def tagable_words(self):\n",
    "        return [(w, pos) for w,  pos , _  in self.tokens if re.search(r\"(NN)|(VB)\", pos)]\n",
    "    \n",
    "    def important_word(self):\n",
    "        return \" \".join([w for w,  pos , _  in self.tokens if re.search(r\"(NN)|(VB)|(JJ)|(CD)\", pos) ])\n",
    "    \n",
    "    def important_lemma(self):\n",
    "        return \" \".join([l for _,  pos , l  in self.tokens if re.search(r\"(NN)|(VB)|(JJ)|(CD)\", pos) ])\n",
    "    \n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "def chunk_text(tagged):\n",
    "    chunks = parser.parse(tagged)\n",
    "    index = 0\n",
    "    segments = []\n",
    "    for el in chunks:\n",
    "        if type(el) == tree.Tree:\n",
    "            chunk = Chunk(el.label(), el.leaves(), list(range(index, index + len(el.leaves()))))\n",
    "            segments.append(chunk)\n",
    "            index += len(el.leaves())\n",
    "        else:\n",
    "            index += 1\n",
    "    return segments\n",
    "\n",
    "def extract_phrase(sentences, merge_inplace=False):\n",
    "    chunks = []\n",
    "    for sentence in sentences:\n",
    "        chunks.append(chunk_text(sentence))\n",
    "    if merge_inplace:\n",
    "        return [merge_adjacent_chunks(chunk) for chunk in chunks]\n",
    "    return chunks  \n",
    "\n",
    "def merge_adjacent_chunks(chunks):\n",
    "    merged = []\n",
    "    previous_label = \"\"\n",
    "    for chunk in chunks:\n",
    "        if chunk.label == previous_label and chunk.label != \"prep_noun\":\n",
    "            merged[-1] = Chunk(chunk.label, \n",
    "                               merged[-1].tokens + chunk.tokens, \n",
    "                               merged[-1].indices + chunk.indices)\n",
    "        else:\n",
    "            merged.append(chunk)\n",
    "        previous_label = chunk.label\n",
    "    return merged\n",
    "\n",
    "def compute_combinations(sentences, n):\n",
    "    return [chunks[i:i+n] for chunks in sentences for i in range(len(chunks)-(n-1))]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wordnet_cats(chunk):\n",
    "    return [(word, tag, wordnet_category(word, tag)) for  word,tag in chunk.tagable_words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute linguistic pattern combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wordnet_tags_per_chunk(sentences, wordnet_tags):\n",
    "     for sent in sentences:\n",
    "        for chunk in sent:\n",
    "            tagged_words = extract_wordnet_cats(chunk)\n",
    "            for word,pos,wordnet_tag in tagged_words:\n",
    "                if wordnet_tag not in wordnet_tags.keys():\n",
    "                    wordnet_tags[wordnet_tag] = Counter()\n",
    "                wordnet_tags[wordnet_tag][word.lower()] +=1\n",
    "\n",
    "def compute_linguistic_patterns(df_series, n):\n",
    "    pattern_dictionary = {}\n",
    "    wordnet_tags = {}\n",
    "\n",
    "    for vals in tqdm_notebook(df_series.values):\n",
    "        sents = extract_phrase(vals, True)\n",
    "        compute_wordnet_tags_per_chunk(sents, wordnet_tags)\n",
    "                            \n",
    "        for combo in compute_combinations(sents, n):\n",
    "            key = tuple([c.label for c in combo])\n",
    "            counter_key =  tuple([c.text.lower() for c in combo])\n",
    "            \n",
    "            if key not in pattern_dictionary.keys():\n",
    "                pattern_dictionary[key]=Counter()\n",
    "\n",
    "            pattern_dictionary[key][counter_key]+=1\n",
    "                        \n",
    "    return pattern_dictionary, wordnet_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet categorization of individual arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos):\n",
    "    if pos.startswith(\"NN\"):\n",
    "        return wn.NOUN\n",
    "    if pos.startswith(\"VB\"):\n",
    "        return wn.VERB\n",
    "    if pos.startswith(\"RB\"):\n",
    "        return wn.ADV\n",
    "    if pos.startswith(\"JJ\"):\n",
    "        return wn.ADJ\n",
    "    \n",
    "def wordnet_category(word, pos):\n",
    "    wn_pos = get_wordnet_pos(pos)\n",
    "    if re.search(\"[cCo]((rona)|(vid))\", word):\n",
    "        return \"noun.state\"\n",
    "    if len(word.split(\" \")) > 1:\n",
    "        word = word.split(\" \")[-1]\n",
    "    if len(wn.synsets(word, wn_pos))>0 :\n",
    "        syns = wn.synsets(word, wn_pos)\n",
    "#         [syn.lexname() for syn in syns]\n",
    "        return syns[0].lexname()\n",
    "    return \"?\"\n",
    "\n",
    "def bulk_compute_categories(argument_list):\n",
    "    counter = Counter()\n",
    "    for argument, counts in argument_list.items():\n",
    "        wordnet_cat = wordnet_category(argument, \"NN\")\n",
    "        if \"Tops\" in wordnet_cat:\n",
    "            wordnet_cat = f\"noun.{argument.lower().split(' ')[-1]}\"\n",
    "        counter[wordnet_cat] += counts\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_categories(word, pos):\n",
    "    wn_pos = get_wordnet_pos(pos)\n",
    "    if re.search(\"[cCo]((rona)|(vid))\", word):\n",
    "        return \"noun.state\"\n",
    "    if len(word.split(\" \")) > 1:\n",
    "        word = word.split(\" \")[-1]\n",
    "    if len(wn.synsets(word, wn_pos))>0 :\n",
    "        syns = wn.synsets(word, wn_pos)\n",
    "#         [syn.lexname() for syn in syns]\n",
    "        return [syn.lexname() for syn in syns]\n",
    "    return \"?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expression matches for themes of interest.\n",
    "Focusing tagging verbs and tagging second argument component of verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_for_theme(text):\n",
    "    if re.search(r\"self\\s?(-|\\s)\\s?employ\", text.lower()):\n",
    "        return \"self-employ\"\n",
    "    if re.search(r\"(deliver(y|(ies)|(ed)))|(slot)|(online shopping)\", text.lower()):\n",
    "        return \"delivery\"\n",
    "    if re.search(r\"vulnerable\", text.lower()):\n",
    "        return \"vulnerable\"\n",
    "    if re.search(r\"disab((led)|(ility))\", text.lower()):\n",
    "        return \"disabled\"\n",
    "    if re.search(r\"no symptom\", text.lower()):\n",
    "        return \"no-symptoms\"\n",
    "    if re.search(r\"((corona)?(virus))|(covid)\", text.lower()):\n",
    "        return \"covid-mention\"\n",
    "    if re.search(r\"\"\"((health)|(heart) (problem)|(issue)|(condition)|(attack)|(disease)|(failure))|( ms)|\"\"\"+\n",
    "                 \"\"\"(copd)|(asthma)|((type)\\s?[12])|(diabet)|\"\"\"+\n",
    "                 \"\"\"(cancer)|(dementia)|(stroke)|(illness)|(a type$)|(cough)|(leukaemia)\"\"\", text.lower()):\n",
    "        return \"health-problem\"\n",
    "    if re.search(r\"symptom\", text.lower()):\n",
    "        return \"symptoms\"\n",
    "    if re.search(r\"((at)?(\\s(very\\s)?high)?\\srisk)|(risk list)\", text.lower()):\n",
    "        return \"at-risk\"\n",
    "    if re.search(r\"\"\"((((a|'|’)m( (in|at)( my)?)?)|aged) (over(-|\\s))?\"\"\"+\n",
    "                 \"\"\"(([789][0-9]($|s|\\s))|(old)|(elderly)))|((over(-|\\s))?[789][0-9] y)\"\"\", text.lower()):\n",
    "        return \"elderly\"\n",
    "    if re.search(r\"(carer)|(care home)\", text.lower()):\n",
    "        return \"carer\"\n",
    "    if re.search(r\"(key\\s?(\\s|-)?\\s?worker)|(nurse($|\\s))|(essential worker)\", text.lower()):\n",
    "        return \"key-worker\"\n",
    "    if re.search(r\"can\\s?(no|'|’)?t work\", text.lower()):\n",
    "        return \"cannot-work\"\n",
    "    if re.search(r\"no ((work)|(income)|(money)|(wage)|(salar))\", text.lower()):\n",
    "        return \"no-income\"\n",
    "    if re.search(r\"(furlough)|(fired)|(80 %)\", text.lower()):\n",
    "        return \"laid-off\"\n",
    "    if re.search(r\"\"\"(((can\\s?(no|'|’)?t (get|buy|(shop for)))|\"\"\"+\n",
    "                 \"\"\"((do not)?ha(ve|d) )(no|any|(not enough))?) (food|groceries))\"\"\", \n",
    "                 text.lower()):\n",
    "        return \"cannot-get-food\"\n",
    "    if re.search(r\"can\\s?(no|'|’)?t get ((med)|(prescription))\", text.lower()):\n",
    "        return \"cannot-get-med\"\n",
    "    if re.search(r\"(^med)|(prescription)\", text.lower()):\n",
    "        return \"get-med\"\n",
    "    if re.search(r\"(travel(\\s(advi[sc]e)|(status))?)|(flight)|(destination)\", text.lower()):\n",
    "        return \"travel\"\n",
    "    if re.search(r\"\"\"(no\\s)(\\w*\\s)?((info)|(clarification)|(advi[sc]e)|((contact )?((details)|(number)))|\"\"\"+\n",
    "                 \"\"\"(answer)|(update)|(clarity)|(guid(e|(ance)))|(list)|(definition)|\"\"\"+\n",
    "                 \"\"\"(address)|(link)|(form)|(contact)|(mention))\"\"\"\n",
    "                 , text.lower()):\n",
    "        return \"no-information\"\n",
    "    if re.search(r\"\"\"(info)|(clarification)|(advi[sc]e)|((contact )?((details)|(number)))|\"\"\"+\n",
    "                 \"\"\"(answer)|(update)|(clarity)|(guid(e|(ance)))|(list)|(definition)|\"\"\"+\n",
    "                 \"\"\"(address)|(link)|(form)|(contact)\"\"\"\n",
    "                 , text.lower()):\n",
    "        return \"information\"\n",
    "    if re.search(r\"\"\"(no)\\s((letter)|(t(e)?xt)|(message)|(e(\\s|(\\s?-\\s?))?mail)|\"\"\"+\n",
    "                 \"\"\"(alert)|(notice)|(communication))\"\"\", text.lower()):\n",
    "        return \"no-correspondence\"\n",
    "    if re.search(r\"(letter)|(t(e)?xt)|(message)|(e(\\s|(\\s?-\\s?))?mail)|(alert)|(notice)\", text.lower()):\n",
    "        return \"correspondence\"\n",
    "    if re.search(r\"(no\\s?((family)|(one)))|(nothing)|(nobody)\", text.lower()):\n",
    "        return \"no-one\"\n",
    "    if re.search(r\"no ((support)|(aid)|(help)|(assistance)|(access)|(priority))\", text.lower()):\n",
    "        return \"no-support\"\n",
    "    if re.search(r\"(support)|(aid)|(help)|(assistance)|(access)|(priority)\", text.lower()):\n",
    "        return \"support\"\n",
    "    if re.search(r\"(child)|((^|\\s)son)|(daughter)\", text.lower()):\n",
    "        return \"child\"\n",
    "    if re.search(r\"\"\"(parent)|(husband)|(wife)|(partner)|\"\"\"+\n",
    "                 \"\"\"((mo|fa)ther)|(famil(y|(ies)))|(m[uo]m)|(dad)\"\"\", text.lower()):\n",
    "        return \"family\"\n",
    "    if re.search(r\"(rule)|(restriction)|(measure)|(rights)\", text.lower()):\n",
    "        return \"rules\"\n",
    "    if re.search(r\"((no)|(a(ny)?)) ((way)|(option)|(choice)|(means)|(idea))\", text.lower()):\n",
    "        return \"uncertainty\"\n",
    "    if re.search(r\"work ((for)|(in)|(at)|(on))\", text.lower()):\n",
    "        return \"work\"\n",
    "    if re.search(r\"((self\\s|-)?isolat((ion)|(e)|(ing)))|(lock\\s?(\\s|-)?\\s?down)\", text.lower()):\n",
    "        return \"self-isolation\"\n",
    "    if re.search(r\"(driv(ing|ers)\\s)?licen[sc]e\", text.lower()):\n",
    "        return \"license\"\n",
    "    if re.search(r\"passport\", text.lower()):\n",
    "        return \"passport\"\n",
    "    if re.search(r\"pension\", text.lower()):\n",
    "        return \"pension\"\n",
    "    if re.search(r\"(^|\\s)h((ome)|(ouse))\", text.lower()):\n",
    "        return \"home-mention\"\n",
    "    if re.search(r\"(employ)|(work)|(job)|(business)|(company)\", text.lower()):\n",
    "        return \"work-mention\"\n",
    "    if re.search(r\"(benefit)|(universal credit)|(eligible)|(esa)|(ssp)|(pip)|(allowance)\", text.lower()):\n",
    "        return \"benefit\"\n",
    "    if re.search(r\"(school)|(student)\", text.lower()):\n",
    "        return \"school\"\n",
    "    if re.search(r\"(food)|(supplies)|(shopping)|(groceries)\", text.lower()):\n",
    "        return \"goods\"\n",
    "    if re.search(r\"(money)|(grant)|(fund)|(relief)\", text.lower()):\n",
    "        return \"given-money\"\n",
    "    if re.search(r\"(bill)|(tax)|(mortgage)|(rent)|(loan)|(debt)|(fine)|(fee)|(insurance)\", text.lower()):\n",
    "        return \"bills-to-pay\"\n",
    "    if re.search(r\"scheme\", text.lower()):\n",
    "        return \"scheme\"\n",
    "    if re.search(r\"(^|\\s)visa($|\\s)\", text.lower()):\n",
    "        return \"visa\"\n",
    "    if re.search(r\"(data)|(cases)|(situation)|(stat(istic)?s?$)|(status)|(news)|(progress)\", text.lower()):\n",
    "        return \"data\"\n",
    "    if re.search(r\"dea((th)|d)\", text.lower()):\n",
    "        return \"death\"\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_for_theme(\"stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_group_verbs(verb):\n",
    "    if re.search(r\"\"\"(f(i|(ou))nd)|(look)|(search)|(clarify)|(ask)|(read)|([ei]nquire)|\"\"\"+\n",
    "                 \"\"\"(obtain)|(seek)|(know)|((^|\\s)see($|\\s))|(understand)\"\"\", verb):\n",
    "        return \"find-smthg\"\n",
    "    if re.search(r\"(access)|(check)|(complete)|(cancel)|(book)|(confirm)\", verb):\n",
    "        return \"access-smthg\"\n",
    "    if re.search(r\"(get)|(take)|(claim)|(receive)|(sent)|(collect)\", verb):\n",
    "        return \"acquire-smthg\"\n",
    "    if re.search(r\"(renew)|(change)|(update)|(inform$)|(notify)\", verb):\n",
    "        return \"change-smthg\"\n",
    "    if re.search(r\"(appl(y|(ied)))|(register)|(qualify)\", verb):\n",
    "        return \"apply-smthg\"\n",
    "    if re.search(r\"pa(y|(id)|(yed))\", verb):\n",
    "        return \"pay-smthg\"\n",
    "    if re.search(r\"(contact)|(report)\", verb):\n",
    "        return \"contact-smthg\"\n",
    "    if re.search(r\"(work)|(employ)\", verb):\n",
    "        return \"work-smwhr\"\n",
    "    if re.search(r\"(need)|(want)|(require)|(request)|(would like)|(order)\", verb):\n",
    "        return \"need-smthg\"\n",
    "    if re.search(r\"(have)|((a|'|’|^)m($|\\s))|(feel($|\\s))\", verb):\n",
    "        return \"my-situation\"\n",
    "    if re.search(r\"(has)|(((a|we)|'|’|^)re($|\\s))\", verb):\n",
    "        return \"others-situation\"\n",
    "    if re.search(r\"(had)|((i|'|’|^)s($|\\s))|(was)\", verb):\n",
    "        return \"unclear-situation\"\n",
    "    if re.search(r\"travel\", verb):\n",
    "        return \"travel\"\n",
    "    if re.search(r\"(liv(e|(ing)))|(stay)\", verb):\n",
    "        return \"living\"\n",
    "    if re.search(r\"(do)|(make)\", verb):\n",
    "        return \"do-smthng\"\n",
    "    if re.search(r\"go($|\\s)\", verb):\n",
    "        return \"go-smwhr\"\n",
    "    if re.search(r\"(give)|(provide)\", verb):\n",
    "        return \"give-smthng\"\n",
    "    if re.search(r\"(help)|(protect)|(support)\", verb):\n",
    "        return \"help\"\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[7]\n",
    "example = df[df.Q3.str.contains(\"shopping advice\")].iloc[0]\n",
    "print(f\"Themetatic category for entire comment: {regex_for_theme(example.Q3)}\")\n",
    "\n",
    "print(example.Q3)\n",
    "print()\n",
    "print(example.pos_tag)\n",
    "print()\n",
    "for sent in extract_phrase(example.pos_tag, True):\n",
    "    for chunk in sent:\n",
    "        print(\"{0:10} {1:20} {2} {3}\".format(chunk.label.upper(), chunk.text, chunk.indices, \n",
    "                                             extract_wordnet_cats(chunk)))\n",
    "    print()\n",
    "    for combo in compute_combinations([sent], 2):\n",
    "        print(f\"{combo[0].text}, {combo[1].text}\")\n",
    "        \n",
    "#     for combo in compute_combinations([sent], 3):\n",
    "#         print(f\"{combo[0].text}, {combo[1].text}, {combo[2].text}\")\n",
    "    print(\"=====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect arg1-arg2 grammatical patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_d, wordnet_tags = compute_linguistic_patterns(df.pos_tag, 2)\n",
    "pattern_d.keys(), len(pattern_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in sorted(wordnet_tags.items(), \n",
    "                        key = lambda x: sum(x[1].values()), \n",
    "                        reverse=True):\n",
    "    print(key, sum(value.values()))\n",
    "    for i,(k,v) in enumerate(value.most_common(20),1):\n",
    "        print(f\"{i}. {k}: {v}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (k,v) in enumerate(sorted(pattern_d.items(), \n",
    "#                                  key = lambda x: sum(x[1].values()), \n",
    "#                                  reverse=True), \n",
    "#                           1):\n",
    "#     print(f\"{i}. {k}: {sum(v.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k,v) in enumerate(sorted(pattern_d.items(), \n",
    "                                 key = lambda x: len(x[1]), \n",
    "                                 reverse=True), \n",
    "                          1):\n",
    "    print(f\"{i}. {' - '.join([ks.upper() for ks in k])} : {len(v)}\\n-------------\")\n",
    "    for j, (kk,vv) in enumerate(pattern_d[k].most_common(50), 1):\n",
    "        print(f\"{j}. \\'{' '.join([f'[{kks}]' for kks in kk])}\\' : {vv}\")\n",
    "    print()\n",
    "    print(\"=======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_of_interest = [('verb', 'noun'),\n",
    "('noun', 'prep_noun'),\n",
    "('prep_noun', 'prep_noun'),\n",
    "('verb', 'noun_verb'),\n",
    "('verb', 'prep_noun'),\n",
    "('noun', 'noun_verb'),\n",
    "('noun_verb', 'prep_noun')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "for i, (k,v) in enumerate(sorted(pattern_d.items(), \n",
    "                                 key = lambda x: len(x[1]), \n",
    "                                 reverse=True), \n",
    "                          1):\n",
    "    if k in patterns_of_interest:\n",
    "#         print(f\"{i}. {' - '.join([ks.upper() for ks in k])} : {len(v)}\\n-------------\")\n",
    "        for j, (kk,vv) in enumerate(pattern_d[k].most_common(10), 1):\n",
    "            if regex_for_theme(\" \".join(kk)) == \"no-income\":\n",
    "                print(regex_for_theme(\" \".join(kk)))\n",
    "                print(f\"{j}. \\'{' '.join([f'[{kks}]' for kks in kk])}\\' : {vv}\")\n",
    "#                 counter+=vv\n",
    "        print()\n",
    "        print(\"=======\\n\")\n",
    "# counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute `arg1` - `arg2` co-occurrence db - couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_db = {}\n",
    "\n",
    "for vals in tqdm_notebook(df.pos_tag.values):\n",
    "    sents = extract_phrase(vals, True)\n",
    "    for combo in compute_combinations(sents, 2):\n",
    "        key = (combo[0].label, combo[1].label)\n",
    "        arg1 = combo[0].text.lower()\n",
    "        arg2 = combo[1].text.lower()\n",
    "#         arg2 = \" \".join([w.lower() for w,_ in combo[1].tagable_words()])\n",
    "        \n",
    "        if key not in pattern_db.keys():\n",
    "            pattern_db[key] = {}\n",
    "        if arg1 not in pattern_db[key].keys():\n",
    "            pattern_db[key][arg1] = Counter()\n",
    "            \n",
    "        pattern_db[key][arg1][arg2]+=1\n",
    "\n",
    "print(f\"There are {len(pattern_db)} possible grammatical combos.\")\n",
    "for i, (k,v) in enumerate(sorted(pattern_db.items(),\n",
    "                         key = lambda x: len(x[1].values()),\n",
    "                         reverse= True)[0:15],\n",
    "                                 1):\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_verbs = [key.lower() for key, value in sorted(pattern_db[('verb', 'noun')].items(), \n",
    "                         key = lambda x: sum(x[1].values()), \n",
    "                         reverse= True)[0:100]]\n",
    "counter = 0\n",
    "for verb in top_100_verbs:\n",
    "    if regex_group_verbs(verb)== \"unknown\":\n",
    "        counter+=1\n",
    "        print(counter, verb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_argument_theme_dictionary(dict_new, dict_old):\n",
    "    for theme, value in dict_new.items():\n",
    "        if theme not in dict_old.keys():\n",
    "            dict_old[theme] = Counter()\n",
    "        for val,count in value.items():\n",
    "            dict_old[theme][val]+=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ('verb', 'prep_noun')\n",
    "# verbs = []patterns_of_interest\n",
    "verb_themes = {}\n",
    "verb_argument_themes = {}\n",
    "\n",
    "for pattern in [('verb', 'noun'), ('verb', 'prep_noun')]:\n",
    "    print(f\"There are {len(pattern_db[pattern])} {pattern[0]}s, accompanied by {pattern[1]}s.\")\n",
    "    for i, (arg1, arg2) in enumerate(sorted(pattern_db[pattern].items(),\n",
    "                             key = lambda x: sum(x[1].values()),\n",
    "                             reverse= True),\n",
    "                                     1):\n",
    "        verb_theme = f\"{regex_group_verbs(arg1)}\".upper()\n",
    "\n",
    "        if verb_theme not in verb_themes.keys():\n",
    "            verb_themes[verb_theme] = Counter()\n",
    "        \n",
    "        verb_themes[verb_theme][arg1] += sum(arg2.values())  \n",
    "        \n",
    "#         print(f\"{i}. {arg1} :: {sum(arg2.values())} [{verb_theme}] \\n-----------\")\n",
    "        \n",
    "        if verb_theme not in verb_argument_themes.keys():\n",
    "            verb_argument_themes[verb_theme] = {}\n",
    "\n",
    "        local_themes = {}\n",
    "        \n",
    "        for j, (arg2_val, arg2_counts) in enumerate(arg2.items(), 1):\n",
    "            theme = f\"{regex_for_theme(arg2_val)}\".upper()\n",
    "            if theme not in local_themes.keys():\n",
    "                local_themes[theme] = Counter()\n",
    "            local_themes[theme][arg2_val]+=arg2_counts   \n",
    "            \n",
    "        update_argument_theme_dictionary(local_themes, verb_argument_themes[verb_theme])\n",
    "#             print(f\"{j}. {arg2_val} : {arg2_counts} [{regex_for_theme(arg1 +' '+arg2_val)}]\")\n",
    "#         for l, (key,value) in enumerate(sorted(local_themes.items(),\n",
    "#                              key = lambda x: sum(x[1].values()),\n",
    "#                              reverse= True)[0:10],\n",
    "#                                      1):\n",
    "#             print(f\"{l}. {key}:: {sum(value.values())}\")\n",
    "#             for argument, count in value.most_common(5):\n",
    "#                 print(f\"{argument}: {count}\")\n",
    "#             print(\"\")\n",
    "#         print(\"=======\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, (verb_type, verb_values) in enumerate(sorted(verb_themes.items(),\n",
    "                             key = lambda x: sum(x[1].values()),\n",
    "                             reverse= True),\n",
    "                                     1):\n",
    "    print(i, verb_type, sum(verb_values.values()), len(verb_values))\n",
    "    \n",
    "#     for verb_value, count in verb_values.most_common(10):\n",
    "#         print(verb_value, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (key,value) in enumerate(sorted(verb_argument_themes.items(),\n",
    "#                              key = lambda x: sum(x[1].values()),\n",
    "#                              reverse= True),\n",
    "#                                      1):\n",
    "for i, (key,value) in enumerate(sorted(verb_argument_themes.items(),\n",
    "                                       key = lambda x: sum([sum(counter.values()) for counter in x[1].values()]),\n",
    "                                      reverse=True),1):\n",
    "    print(f\"{i}. {key} {sum([sum(counter.values()) for counter in value.values()])} \\n======\")\n",
    "#     if k == \"UNKNOWN\"\n",
    "    for j, (argument, counter) in enumerate(sorted([(k,v) for k,v in value.items() ],\n",
    "                                                   key = lambda x: sum(x[1].values()),\n",
    "                                                   reverse=True\n",
    "                                                  )[0:10]\n",
    "                                            , 1):\n",
    "\n",
    "        print(f\"{j}. {argument}: {sum(counter.values())}\")\n",
    "#         for arg_theme, vals in counter.most_common(5):\n",
    "#             print(f\"{arg_theme}: {vals}\")\n",
    "#         print(\"---\")\n",
    "    print()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign themes to actions and things people are talking about \n",
    "### Tag response comments (Q3) with appropriate themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_mentions = []\n",
    "for vals in tqdm_notebook(df.pos_tag.values):\n",
    "    sents = extract_phrase(vals, True)\n",
    "    phrase_mentions.append([])\n",
    "    for combo in compute_combinations(sents, 2):\n",
    "        key = (combo[0].label, combo[1].label)\n",
    "        arg1 = combo[0].text.lower()\n",
    "        arg2 = combo[1].text.lower()\n",
    "        \n",
    "        if key in [('verb', 'noun'), ('verb', 'prep_noun')]:\n",
    "            mention_theme = f\"{regex_group_verbs(arg1)} - {regex_for_theme(arg2)}\"\n",
    "            phrase_mentions[-1].append((key, f\"{arg1} {arg2}\", mention_theme))\n",
    "            \n",
    "df['theme_mentions'] = phrase_mentions       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['theme_mentions_list'] = df['theme_mentions'].map(lambda x: [mention for key,_, mention in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[0]\n",
    "print(\"text:\", example.Q3)\n",
    "print(\"identified verb-based themed mentions:\", example.theme_mentions)\n",
    "print(\"identified themes:\", example.theme_mentions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ended_date'] = df['Ended'].map(lambda x: \"\".join(x.split(\" \")[0].split(\"/\")[::-1]))\n",
    "index = sorted(df['ended_date'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_index_label(index):\n",
    "    return f\"{index[-2:]}/{index[:2]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention_date = {}\n",
    "for date, mentions in df[['ended_date', 'theme_mentions_list']].values:\n",
    "    for mention in mentions:\n",
    "        if mention not in mention_date.keys():\n",
    "            mention_date[mention] = Counter()\n",
    "        mention_date[mention][date] += 1\n",
    "\n",
    "column_dict = {}\n",
    "for mention,date_counts in sorted([(k,v) for k,v in mention_date.items() if \"unknown\" not in k],\n",
    "                                 key = lambda x: sum(x[1].values()),\n",
    "                                 reverse=True)[2:10]:\n",
    "#     if all([exclude not in mention for exclude in [\"find-smthg\", \"information\"]]):\n",
    "#         if \"situation\" in mention:\n",
    "    print(mention)\n",
    "    column_dict[mention] = [date_counts.get(date, 0) for date in index]\n",
    "\n",
    "maximum_value = max([v for date_counter in [v for k,v in mention_date.items() \n",
    "                                            if k in column_dict.keys()] \n",
    "                     for v in date_counter.values()])\n",
    "df2 = pd.DataFrame(column_dict, index=index)\n",
    "# lines = df2.plot.line(figsize=(20,10))\n",
    "df2.shape, maximum_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_dash(n):\n",
    "    return [(random.randint(2, 8), 0.5, 1, 0.5) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\", font_scale=1.2)\n",
    "plt.figure(figsize=(30, 15))\n",
    "\n",
    "# ax = sns.lineplot(index, 375, color='black', alpha=0.5)\n",
    "# ax.lines[0].set_linestyle(\"--\")\n",
    "# plt.axvline(20200324)\n",
    "\n",
    "palette = sns.color_palette(\"colorblind\", df2.shape[1])\n",
    "ax = sns.lineplot(data=df2, palette=palette, dashes = generate_dash(df2.shape[1]))\n",
    "\n",
    "plt.ylabel(r'# of mentions')\n",
    "plt.xlabel('')\n",
    "\n",
    "# plt.yticks(np.arange(0, maximum_value, step=50))\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), \n",
    "           bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "\n",
    "sns.despine(offset=10)\n",
    "\n",
    "labels = [fix_index_label(ind.replace(\"2020\", \"\")) for ind in index]\n",
    "ax.set_xticklabels(labels, rotation=25)\n",
    "plt.savefig(os.path.join(DATA_DIR, \"exclude_info.png\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching (not very helpful so far) [OLD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = []\n",
    "for k,v in pattern_db.items():\n",
    "    if k in [(\"verb\", \"noun\"), (\"verb\", \"prep_noun\")]:\n",
    "        verbs.extend(list(v.keys()))\n",
    "verbs = list(set(verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect the verbs from the arg1-arg2 combinations\n",
    "t1 = \"can claim\"\n",
    "t2 = \"to claim\"\n",
    "print(fuzz.partial_ratio(t1, t2))\n",
    "print(fuzz.token_sort_ratio(t1, t2))\n",
    "\n",
    "verb_associations = {}\n",
    "for verb in tqdm_notebook(sorted(verbs,\n",
    "                   key = lambda x: len(x.split(\" \")))):\n",
    "    excluded = verbs[:]\n",
    "    excluded.remove(verb)\n",
    "#     if verb == \"can claim\":\n",
    "#         print(process.extractBests(verb, \n",
    "#                                    excluded, \n",
    "#                                    scorer=fuzz.token_sort_ratio))\n",
    "    verb_associations[verb] = process.extractBests(verb, \n",
    "                                                   excluded, \n",
    "                                                   scorer=fuzz.token_sort_ratio, \n",
    "                                                   score_cutoff=80)\n",
    "# verb_associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = {}\n",
    "skip = []\n",
    "for k,v in sorted(verb_associations.items(),\n",
    "                  key = lambda x: len(x[0])):\n",
    "    if k not in skip:\n",
    "        merged[k] = v[:]\n",
    "        for vs in v:\n",
    "            if vs[0] in verb_associations.keys():\n",
    "                skip.append(vs[0])\n",
    "                for vv in verb_associations[vs[0]]:\n",
    "                    if vv[0] not in [val for val,_ in merged[k]] and vv[0] != k:\n",
    "                        merged[k].append(vv)\n",
    "                        skip.append(vv[0])\n",
    "\n",
    "## if values intersect, merge\n",
    "intersection = []\n",
    "for k,v in merged.items():\n",
    "    for m,n in merged.items():\n",
    "        if k!=m:\n",
    "            if len(set([phrase for phrase,_ in v]).intersection(set([phrase2 for phrase2,_ in n]))):\n",
    "                print(f\"{k}, {m}\")\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "rowlist = []\n",
    "for key in [key for key in pattern_d.keys() ]:\n",
    "#     print(key)\n",
    "    for k,v in pattern_d[key].most_common():\n",
    "        if \"delivery\" in k[1] or \"deliver\" in k[0]:\n",
    "#             rowlist.append({f\"\"})\n",
    "            print(f\"{i}. [{k[0]}] {k[1]} : {v}\")\n",
    "            i+=1\n",
    "    print()\n",
    "    i=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_triples = compute_linguistic_patterns(df.pos_tag, 3)\n",
    "\n",
    "for i, (k,v) in enumerate(sorted(pd_triples.items(), \n",
    "                                 key = lambda x: len(x[1]), \n",
    "                                 reverse=True), \n",
    "                          1):\n",
    "    print(f\"{i}. {' - '.join([ks.upper() for ks in k])} : {len(v)}\\n-------------\")\n",
    "    for j, (args, counts) in enumerate(pd_triples[k].most_common(10), 1):\n",
    "        print(f\"{j}. {args}: {counts}\")\n",
    "    print()\n",
    "    print(\"=======\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
