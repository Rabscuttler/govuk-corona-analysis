{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback tagging - modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from functools import partial\n",
    "from gensim.sklearn_api import D2VTransformer, W2VTransformer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src import calculate_metrics, tagging_preprocessing\n",
    "from src.make_feedback_tagging.calculate_performance_metrics import FUNCS_METRICS_PRED\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import xgboost as xgb\n",
    "\n",
    "# Enable multiple cell outputs\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables\n",
    "\n",
    "Define some environmental and constant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract environment variables\n",
    "DIR_DATA_RAW = os.getenv(\"DIR_DATA_RAW\")\n",
    "\n",
    "# Define the raw data file path\n",
    "FILE_RAW = os.path.join(DIR_DATA_RAW, \"20200521 Coronavirus feedback analysis - Tagging sheet.csv\")\n",
    "\n",
    "# Define the column containing the free text for analysis\n",
    "COLS_FREE_TEXT = [\"q3\", \"q8\"]\n",
    "\n",
    "# Define the number of processors in this machine\n",
    "COUNT_CPU = mp.cpu_count()\n",
    "\n",
    "# Define metric functions that use prediction probabilities\n",
    "FUNCS_METRICS_PROB = [partial(roc_auc_score, multi_class=\"ovr\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and process data\n",
    "\n",
    "Import the CSV data, and pre-process it in preparation for model build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the raw data file as a pandas DataFrame\n",
    "df_raw = pd.read_csv(FILE_RAW)\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check tag counts\n",
    "df_raw[\"This response relates to...\"].value_counts(dropna=False)\n",
    "df_raw[\"Coronavirus Theme\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset\n",
    "df_process = tagging_preprocessing(df_raw, COLS_FREE_TEXT)\n",
    "df_process.shape\n",
    "df_process.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline modelling - `this_response_relates_to_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null_and_single_values(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"Remove NULL, and single-occurrence values from a column of a pandas DataFrame.\n",
    "\n",
    "    :param df: A pandas DataFrame.\n",
    "    :param col: A column of in `df`.\n",
    "    :return: A filtered version of `df`, where NULL entries in `col` are removed, as well as any values that only \n",
    "        appear once in `col`.\n",
    "\n",
    "    \"\"\"\n",
    "    return df[df[col].notnull()].groupby(col).filter(lambda x: len(x) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tag we will model first\n",
    "col_target = \"this_response_relates_to_\"\n",
    "\n",
    "# Remove NULL and single occurrence values from `col_target`, and convert it into a category\n",
    "df_model = remove_null_and_single_values(df_process, col_target)\n",
    "df_model = df_model.assign(**{col_target: df_model[col_target].astype(\"category\")})\n",
    "\n",
    "# Print the encoded labels\n",
    "_ = [print(f\"{tag}: {ix}\") for ix, tag in enumerate(df_model[col_target].cat.categories)]\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "lemma_X_train, lemma_X_test, lemma_y_train, lemma_y_test = train_test_split(\n",
    "    df_model[\"lemma\"].values,\n",
    "    df_model[col_target].cat.codes.values,\n",
    "    test_size=0.3,\n",
    "    random_state=42, \n",
    "    stratify=df_model[col_target]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinominal Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model pipeline\n",
    "mnb_pipeline = Pipeline([(\"tfidf\", TfidfVectorizer()),\n",
    "                           (\"clf\", MultinomialNB())])\n",
    "\n",
    "# Fit the model\n",
    "_ = mnb_pipeline.fit(lemma_X_train, lemma_y_train)\n",
    "\n",
    "# Predict the model\n",
    "mnb_y_pred = mnb_pipeline.predict(lemma_X_test)\n",
    "mnb_y_pred_prob = mnb_pipeline.predict_proba(lemma_X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "mnb_metrics = calculate_metrics(lemma_y_test, mnb_y_pred, mnb_y_pred_prob, FUNCS_METRICS_PRED, FUNCS_METRICS_PROB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate a TfidfVectorizer object\n",
    "xgb_tfidf = TfidfVectorizer()\n",
    "\n",
    "# Transform both the training and test sets\n",
    "xgb_X_train = xgb_tfidf.fit_transform(lemma_X_train, lemma_y_train)\n",
    "xgb_X_test = xgb_tfidf.transform(lemma_X_test, lemma_y_test)\n",
    "\n",
    "# Instantiate an XGBClassifier object\n",
    "xgb_clf = xgb.XGBClassifier(objective=\"multi:softmax\", seed=42)\n",
    "\n",
    "# Fit `xgb_clf` using a early stopping\n",
    "_ = xgb_clf.fit(xgb_X_train, lemma_y_train,\n",
    "                eval_set=[(xgb_X_train, lemma_y_train), (xgb_X_test, lemma_y_test)],\n",
    "                eval_metric=\"mlogloss\", early_stopping_rounds=10)\n",
    "\n",
    "# Predict the model\n",
    "xgb_y_pred = xgb_clf.predict(xgb_X_test)\n",
    "xgb_y_pred_prob = xgb_clf.predict_proba(xgb_X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "xgb_metrics = calculate_metrics(lemma_y_test, xgb_y_pred, xgb_y_pred_prob, FUNCS_METRICS_PRED, FUNCS_METRICS_PROB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with Grid Search\n",
    "\n",
    "Warning - can take a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate an XGBClassifier object\n",
    "xgb_gs_clf = xgb.XGBClassifier(objective=\"multi:softmax\", seed=42)\n",
    "\n",
    "\n",
    "# Define a parameter set for the grid search\n",
    "xgb_grid_params = {\"learning_rate\": [0.01, 0.1, 0.5, 0.9],\n",
    "                   \"n_estimators\": [200],\n",
    "                   \"subsample\": [0.3, 0.5, 0.9]}\n",
    "\n",
    "# Build a grid search\n",
    "xgb_grid = GridSearchCV(estimator=xgb_gs_clf, param_grid=xgb_grid_params, cv=5, n_jobs=-1,\n",
    "                        scoring=make_scorer(matthews_corrcoef), verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "_ = xgb_grid.fit(xgb_X_train, lemma_y_train,\n",
    "                 eval_set=[(xgb_X_train, lemma_y_train), (xgb_X_test, lemma_y_test)],\n",
    "                 eval_metric=\"mlogloss\", early_stopping_rounds=10)\n",
    "\n",
    "# Predict the model\n",
    "xgb_gs_y_pred = xgb_grid.predict(xgb_X_test)\n",
    "xgb_gs_y_pred_prob = xgb_grid.predict_proba(xgb_X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "xgb_gs_metrics = calculate_metrics(lemma_y_test, xgb_gs_y_pred, xgb_gs_y_pred_prob, FUNCS_METRICS_PRED,\n",
    "                                   FUNCS_METRICS_PROB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise each piece of feedback to use as the feature set; feedback used here is text that has been stripped of \n",
    "# personally identifiable information (PII), in lowercase, with all free text columns `COLS_FREE_TEXT` compiled \n",
    "# into a single string, and all stopwords and certain symbols removed. Symbols removed are (, ), [, ], +, and *\n",
    "clean_X = df_model[\"clean_text\"].map(word_tokenize).values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "clean_X_train, clean_X_test, clean_y_train, clean_y_test = train_test_split(\n",
    "    clean_X,\n",
    "    df_model[col_target].cat.codes.values,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=df_model[col_target]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a model pipeline\n",
    "d2v_pipeline = Pipeline([(\"d2v\", D2VTransformer(dm=0, size=300, seed=42, workers=COUNT_CPU)),\n",
    "                         (\"clf\", LogisticRegression(max_iter=200, n_jobs=-1, random_state=42))])\n",
    "\n",
    "# Define a parameter set for the grid search\n",
    "d2v_grid_params = {\"d2v__size\": [100, 200, 300],\n",
    "                   \"clf__C\": np.logspace(-5, 5, 20),\n",
    "                   \"clf__penalty\": [\"l1\", \"l2\"]}\n",
    "\n",
    "# Build a grid search\n",
    "d2v_grid = GridSearchCV(estimator=d2v_pipeline, param_grid=d2v_grid_params, cv=5, n_jobs=-1,\n",
    "                        scoring=make_scorer(matthews_corrcoef), verbose=2)\n",
    "\n",
    "# Fit the grid search\n",
    "_ = d2v_grid.fit(clean_X_train, clean_y_train)\n",
    "\n",
    "# Predict the model\n",
    "d2v_gs_y_pred = d2v_grid.predict(clean_X_test)\n",
    "d2v_gs_y_pred_prob = d2v_grid.predict_proba(clean_X_test)\n",
    "\n",
    "# Calculate the metrics\n",
    "d2v_gs_metrics = calculate_metrics(clean_y_test, d2v_gs_y_pred, d2v_gs_y_pred_prob, FUNCS_METRICS_PRED,\n",
    "                                   FUNCS_METRICS_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
