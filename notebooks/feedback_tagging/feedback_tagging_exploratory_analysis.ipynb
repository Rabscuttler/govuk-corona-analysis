{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback tagging - exploratory analysis\n",
    "\n",
    "Explore the manual feedback tagging data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from pandas.testing import assert_frame_equal\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Enable multiple cell outputs\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract environment variables\n",
    "DIR_DATA_RAW = os.getenv(\"DIR_DATA_RAW\")\n",
    "\n",
    "# Define the raw data file path\n",
    "FILE_RAW = os.path.join(DIR_DATA_RAW, \"Coronavirus feedback analysis - Tagging sheet.csv\")\n",
    "\n",
    "# Import the raw data file as a pandas DataFrame\n",
    "df_raw = pd.read_csv(FILE_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial preprocessing\n",
    "\n",
    "Let us do some initial preprocessing of the data. First set all the column headers to lowercase, and replace any punctuation with an underscore (adjacent punctuation will be replaced with a single underscore).\n",
    "\n",
    "Also drop any rows of data that do not have at least one tag in either the `this_response_relates_to_` or `coronavirus_theme` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all column names lower case, and replace any punctuation with an underscore\n",
    "df_preprocess = df_raw.rename(columns=lambda n: re.sub(r\"\\W+\", \"_\", n.lower()))\n",
    "\n",
    "# Drop any rows of data where both the `this_response_relates_to_`, and `coronavirus_theme` columns are NA\n",
    "df_preprocess = df_preprocess[(df_preprocess[\"this_response_relates_to_\"].notnull())|\n",
    "                              (df_preprocess[\"coronavirus_theme\"].notnull())]\n",
    "\n",
    "# Set `text_date` as a datetime column - note there are different datetime formats in this column, hence the \n",
    "# regular expression parsing to remove anything after YYYY-mm-dd HH:MM:SS\n",
    "df_preprocess = df_preprocess.assign(text_date=pd.to_datetime(df_preprocess[\"text_date\"].str.replace(\n",
    "    r\"^(?P<one>\\d{4}\\-\\d{2}\\-\\d{2} \\d{2}\\:\\d{2}\\:\\d{2}).*$\", lambda m: m.group('one'))))\n",
    "\n",
    "# Show the number of rows dropped\n",
    "print(f\"Initial preprocessing has dropped {len(df_raw) - len(df_preprocess):,} rows ({len(df_raw):,} rows -> \"\n",
    "      f\"{len(df_preprocess):,} rows)\")\n",
    "df_preprocess.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check `text_date` and `numeric_date` columns\n",
    "\n",
    "Check that the `text_date`, and `numeric_date` columns show the same date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df_preprocess[\"text_date\"], utc=True).dt.strftime(\"%d/%m/%Y\").equals(df_preprocess[\"numeric_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine unique tags\n",
    "\n",
    "Let us count the number and proportion of unique tags in both the `this_response_relates_to_`, and `coronavirus_theme` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts, and proportions of each unique value in the `this_response_relates_to_` column\n",
    "df_this_response_relates_to_counts = df_preprocess[\"this_response_relates_to_\"].value_counts(dropna=False) \\\n",
    "    .to_frame(\"counts\")\n",
    "df_this_response_relates_to_counts = df_this_response_relates_to_counts.assign(\n",
    "    proportion=df_this_response_relates_to_counts[\"counts\"]/df_preprocess.shape[0]\n",
    ")\n",
    "\n",
    "# Get the counts, and proportions of each COVID-19 theme\n",
    "df_coronavirus_theme_counts = df_preprocess[\"coronavirus_theme\"].value_counts(dropna=False).to_frame(\"counts\")\n",
    "df_coronavirus_theme_counts = df_coronavirus_theme_counts.assign(\n",
    "    proportion=df_coronavirus_theme_counts[\"counts\"]/df_preprocess.shape[0]\n",
    ")\n",
    "\n",
    "df_this_response_relates_to_counts\n",
    "df_coronavirus_theme_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df_raw` is sourced from a Google Sheet. In this Google Sheet, there is a tab `Codes` which has a list of possible tags for both the `this_response_relates_to_`, and `coronavirus_theme` columns; let us see what differences `df_preprocess` has between these stated codes (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw `this_response_relates_to` tags that were specificed in the `Codes` sheet of the original raw data\n",
    "raw_this_response_relates_to = {\"govuk-specific\", \"contact-government\", \"complaint-government\", \"service-problem\",\n",
    "                                \"address-problem\", \"finding-problem\", \"things-problem\", \"compliment\", \"ok\", \"none\"}\n",
    "\n",
    "# Get the raw `coronavirus_themes` tags that were specificed in the `Cods sheet of the original raw data\n",
    "raw_coronavirus_themes = {\"Isolation & quarantine\", \"Treatment, diagnosis & symptoms\", \"Lockdown & rules\",\n",
    "                          \"Vulnerable\", \"Other health\", \"Pregnancy\", \"Food & deliveries\", \"Profiteering\",\n",
    "                          \"Business loans\", \"Education and nursery - parents and carers\",\n",
    "                          \"Education and nursery - staff\", \"Health/social care - staff\", \"Travel & transport - UK\",\n",
    "                          \"Travel & transport - abroad\", \"Livelihood - employee\", \"Livelihood - self-employed\",\n",
    "                          \"Employing others & running a business\", \"Volunteering\", \"Housing\", \n",
    "                          \"Advice is confusing, untrustworthy or not up to date\", \"Privacy concern\", \n",
    "                          \"Data “trustiness” - numbers are unreliable, not up to date or inconsistent\", \"Other\",\n",
    "                          \"Non-COVID\"}\n",
    "\n",
    "# Print the differences between `raw_this_response_relates_to`, and the unique tags we have\n",
    "print(\"'df_this_response_relates_to_counts' has the addition unique tags:\\n\"\n",
    "      f\"\\t{set(df_this_response_relates_to_counts.index) - raw_this_response_relates_to}\\n\")\n",
    "\n",
    "# Print the differences between `raw_coronavirus_themes`, and the unique tags we have\n",
    "print(\"'df_coronavirus_theme_counts' has the addition unique tags:\\n\"\n",
    "      f\"\\t{set(df_coronavirus_theme_counts.index) - raw_coronavirus_themes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify duplicated feedback\n",
    "\n",
    "Let us look at the rows of feedback that have been tagged as `duplicate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique key that combines values in both the `this_response_relates_to_` and `coronavirus_theme` columns\n",
    "df_tag_dup_check = df_preprocess.assign(\n",
    "    key=df_preprocess[\"this_response_relates_to_\"] + \"<-->\" + df_preprocess[\"coronavirus_theme\"]\n",
    ")\n",
    "\n",
    "# Get all the rows where at least `this_response_relates_to_` or `coronavirus_theme` is a duplicate\n",
    "df_tag_dup = df_tag_dup_check.query(\"this_response_relates_to_=='duplicate' or coronavirus_theme=='duplicate'\")\n",
    "df_tag_dup.head()\n",
    "\n",
    "# Find the rows with duplicates\n",
    "df_tag_dup_check.query(\"q3.isin(@df_tag_dup.q3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be three sets of duplicates, but the second set is a set of one value, i.e. there are no duplicates in the data set. I assume this is down to human tagging error.\n",
    "\n",
    "Let us see if there are more duplicate records in the data set that may **not** have been flagged as `duplicate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some columns to exclude from the duplicate check process - we only want to perform deduplication over the \n",
    "# original feedback columns themselves, not any datetime- or taggin-related columns\n",
    "EXCLUDED_COLUMNS = [\"text_date\", \"this_response_relates_to_\", \"coronavirus_theme\", \n",
    "                    \"needs_urgent_attention_of_product_teams\"]\n",
    "\n",
    "# Now define the remaining columns\n",
    "INCLUDED_COLUMNS = [c for c in df_preprocess.columns if c not in EXCLUDED_COLUMNS]\n",
    "\n",
    "# Sort `df_preprocess` by the earliest `text_date`, then flag any duplicated rows\n",
    "bool_duplicated_rows = df_preprocess.sort_values(by=[\"text_date\"]) \\\n",
    "    .duplicated(subset=INCLUDED_COLUMNS, keep=False) \\\n",
    "    .sort_index()\n",
    "\n",
    "# Get the duplicated rows of data\n",
    "df_dup = df_preprocess[bool_duplicated_rows]\n",
    "df_dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more records that appear to be duplicates, but have potentially not been tagged as `duplicate`. Again this is probably due to human error.\n",
    "\n",
    "If the tags for these duplicates (aside from the `duplicate` tag) are different, then we will need to perform some additional pre-processing to ensure we have the correct data set - let us check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First drop any duplicates in `df_dup`, aside from in the `text_date` and `needs_urgent_attention_of_product_teams`\n",
    "# columns. This should give us the unique tags for each of the duplicates\n",
    "df_dup_diff_tags = df_dup.drop_duplicates(\n",
    "    subset=[c for c in df_dup.columns if c not in [\"text_date\"]]\n",
    ")\n",
    "\n",
    "# Drop any of `df_dup_diff_tags` that have a `duplicate` tag in either the `this_response_relates_to_` or \n",
    "# `coronavirus_theme` columns\n",
    "df_dup_diff_tags = df_dup_diff_tags.query(\"this_response_relates_to_ != 'duplicate' or \"\n",
    "                                          \"coronavirus_theme != 'duplicate'\")\n",
    "\n",
    "# Now let us extract only those that have different tags - all duplicates with the same tags should now be removed\n",
    "df_dup_diff_tags = df_dup_diff_tags[df_dup_diff_tags.duplicated(subset=INCLUDED_COLUMNS, keep=False)]\n",
    "\n",
    "df_dup_diff_tags.shape\n",
    "df_dup_diff_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us develop a hierarchy to determine which tag to use if a piece of feedback has multiple tags, as in the seven cases of feedback above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a reminder, here are the common `this_response_relates_to_`, and `coronavirus_theme` tags\n",
    "set(df_this_response_relates_to_counts.index) & set(df_coronavirus_theme_counts.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we will define our hierarchy for choosing one amongst duplicated rows:\n",
    "\n",
    "1. Any tag that is not np.nan, i.e. unfilled;\n",
    "2. Any tag that is not `duplicate`;\n",
    "3. Any tag that is not `INTERNAL`;\n",
    "4. Any tag that is not `none` - only applies for the `this_response_relates_to_` column;\n",
    "5. Any tag that is not `ok`; and\n",
    "6. If we still need to break duplicates, then get the earliest datetime (`text_date` column) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a hierarchy of certain `this_response_relates_to_`, and `coronavirus_theme` tags; the keys are the tags, \n",
    "# and the values are their 'rank' in descending order, e.g. np.nan is less desirable than `duplicate`. Although \n",
    "# 'none' does not appear in `coronavirus_theme`, it is still useful to rank this\n",
    "ORDER_TAG = {np.nan: -5, \"duplicate\": -4, \"INTERNAL\": -3, \"none\": -2, \"ok\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all boolean series for any rows that have duplicates\n",
    "bool_duplicate_rows = df_preprocess[INCLUDED_COLUMNS].duplicated(keep=False)\n",
    "\n",
    "# Get the duplicated rows from `df_preprocess`\n",
    "df_preprocess_duplicated = df_preprocess[bool_duplicate_rows]\n",
    "\n",
    "# Rank the datetime for these duplicated rows with earliest datetime last; in descending order, this means the \n",
    "# earliest datetime has the largest rank value\n",
    "s_duplicate_rows_date_ranked = df_preprocess_duplicated[\"text_date\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "# Build a `rank` column where `s_duplicate_rows_date_ranked` is used if the `this_response_relates_to_` is not in the\n",
    "# keys of `ORDER_TAG`. Otherwise, use the value from the mapped dictionary `ORDER_TAG`. This produces a pandas Series \n",
    "# with the earliest date having the largest number, unless it maps to `ORDER_TAG`\n",
    "df_preprocess_duplicated = df_preprocess_duplicated.assign(\n",
    "    rank_this_response_relates_to=s_duplicate_rows_date_ranked.where(\n",
    "        ~df_preprocess_duplicated[\"this_response_relates_to_\"].isin(ORDER_TAG.keys()),\n",
    "        df_preprocess_duplicated[\"this_response_relates_to_\"].replace(ORDER_TAG)).astype(int),\n",
    "    rank_coronavirus_theme=s_duplicate_rows_date_ranked.where(\n",
    "        ~df_preprocess_duplicated[\"coronavirus_theme\"].isin(ORDER_TAG.keys()),\n",
    "        df_preprocess_duplicated[\"coronavirus_theme\"].replace(ORDER_TAG)).astype(int)\n",
    ")\n",
    "\n",
    "# Scale the ranks in `df_preprocess_duplicated` so that the minimum value is 1, if the minimum value is less than 0\n",
    "for col_rank in [\"rank_this_response_relates_to\", \"rank_coronavirus_theme\"]:\n",
    "    if df_preprocess_duplicated[col_rank].min() < 0:\n",
    "        df_preprocess_duplicated = df_preprocess_duplicated.assign(**{\n",
    "            col_rank: df_preprocess_duplicated[col_rank] + abs(df_preprocess_duplicated[col_rank].min()) + 1\n",
    "        })\n",
    "\n",
    "    \n",
    "# Create a combined rank using the rank product statistic (geometric mean of the ranks)\n",
    "df_preprocess_duplicated = df_preprocess_duplicated.assign(\n",
    "    rank=np.sqrt(df_preprocess_duplicated[\"rank_this_response_relates_to\"].multiply(\n",
    "        df_preprocess_duplicated[\"rank_coronavirus_theme\"]))\n",
    ")\n",
    "\n",
    "# Sort in descending order for the `rank` value first (largest number first), and then alphabetically by all other \n",
    "# columns in `INCLUDED_COLUMNS`. Then drop duplicates, keeping only the first entry of any duplicates\n",
    "df_preprocess_duplicated_ranked = df_preprocess_duplicated \\\n",
    "    .sort_values(by=[\"rank\", *INCLUDED_COLUMNS], ascending=[False, *[True] * len(INCLUDED_COLUMNS)]) \\\n",
    "    .drop_duplicates(subset=INCLUDED_COLUMNS)\n",
    "df_preprocess_duplicated_ranked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a finalised, cleaned data set\n",
    "df_out = pd.concat([df_preprocess[~bool_duplicate_rows], df_preprocess_duplicated_ranked[df_preprocess.columns]]) \\\n",
    "    .sort_index()\n",
    "\n",
    "# Check if there are any 'duplicate' tags - should be only the one we couldn't find other duplicates for\n",
    "df_out.query(\"this_response_relates_to_ == 'duplicate' or coronavirus_theme == 'duplicate'\")\n",
    "\n",
    "# Assert that there are no duplicates in the data\n",
    "assert_frame_equal(df_out, df_out.drop_duplicates(subset=INCLUDED_COLUMNS)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably drop this one remaining `duplicate` tag row, as we do not want to tag duplicate data in our machine learning model - this should be straightforward to do without any modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
