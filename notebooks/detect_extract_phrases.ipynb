{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wordcloud\n",
    "# !pip3 install polyglot\n",
    "# !pip3 install pyicu\n",
    "# !pip3 install pycld2\n",
    "# !pip3 install morfessor\n",
    "# !pip3 install polyglot\n",
    "# !pip3 install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np \n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, RegexpParser, tree\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import Counter\n",
    "import re\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import string \n",
    "\n",
    "## https://markhneedham.com/blog/2017/11/28/python-polyglot-modulenotfounderror-no-module-named-icu/\n",
    "from polyglot.detect import Detector\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "\n",
    "survey_filename = os.path.join(DATA_DIR, \"joined_uis_all_of_march.csv\")\n",
    "df = pd.read_csv(survey_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some row duplication present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape, df.intents_clientID.nunique(), df.primary_key.nunique(), df.session_id.nunique())\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df[df.session_id.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"primary_key\", inplace = True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for sentence tokenization, part of speech tagging, PII placeholder stripping, ngram computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "pii_filtered = [\"DATE_OF_BIRTH\", \"EMAIL_ADDRESS\", \"PASSPORT\", \"PERSON_NAME\", \n",
    "                \"PHONE_NUMBER\", \"STREET_ADDRESS\", \"UK_NATIONAL_INSURANCE_NUMBER\", \"UK_PASSPORT\"]\n",
    "pii_regex = \"|\".join([f\"\\\\[{p}\\\\]\" for p in pii_filtered])\n",
    "pii_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation) + ['’']\n",
    "token_blacklist = stop_words + punctuation + pii_filtered\n",
    "\n",
    "def split_sentences(comment):\n",
    "    return nltk.sent_tokenize(comment)\n",
    "\n",
    "def remove_stopwords_punctation(sentences):\n",
    "    return [[(t[0], t[1], t[2]) for t in sent if t[0].lower() not in token_blacklist] for sent in sentences]\n",
    "\n",
    "def replace_pii_regex(text):\n",
    "    return re.sub(pii_regex, \"\", text)\n",
    "\n",
    "def compute_ngrams(processed_comment, n, stemming=False, filtering=False):\n",
    "    # processed_comment = part_of_speech_tag(comment)\n",
    "    if filtering:\n",
    "        processed_comment = remove_stopwords_punctation(processed_comment)\n",
    "    index = 2 if stemming else 0\n",
    "    tokens = [token[index] for sent in processed_comment for token in sent]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram).lower() for ngram in ngrams]\n",
    "\n",
    "\n",
    "def part_of_speech_tag(comment):\n",
    "    sentences = split_sentences(comment)\n",
    "    return [[(token.text, token.tag_, token.lemma_) for token in nlp(sentence)] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"This is a test with punctuation’. this is another sentence.\"\n",
    "processed_t = part_of_speech_tag(t)\n",
    "compute_ngrams(processed_t, 2, stemming=False, filtering=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect feedback language\n",
    "There is a bit of foreign language spam in some responses, detect non (primarily) english comments and drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    if text!=\"-\":\n",
    "        try:\n",
    "            langs = {language.confidence:language.code for language in Detector(text, quiet=True).languages}\n",
    "            return langs[max(langs.keys())]\n",
    "        except:\n",
    "            return f\"[ERROR] {text}\"\n",
    "    return \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Q3_pii_removed'] = df['Q3'].progress_map(replace_pii_regex)\n",
    "df = df[(df.Q3_pii_removed.str.len()<4000)]\n",
    "df['language'] = df['Q3_pii_removed'].progress_map(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dist = df['language'].value_counts().to_dict()\n",
    "print(f\"Number of unique languages: {len(lang_dist)}\")\n",
    "print(f\"English: {round((lang_dist['en']*100)/sum(lang_dist.values()), 2)}%\")\n",
    "print(f\"-: {round((lang_dist['-']*100)/sum(lang_dist.values()), 2)}%\")\n",
    "list(lang_dist.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_en'] = df['language'].isin([\"en\", \"un\", \"-\", \"sco\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tag'] = df[['Q3_pii_removed', 'is_en']].progress_apply(lambda x: part_of_speech_tag(x[0]) \n",
    "                                                     if x[1] else [], axis=1)\n",
    "df['lemmas'] = df['pos_tag'].progress_map(lambda x: [token[2] for sent in x for token in sent])\n",
    "\n",
    "df['words'] = df['pos_tag'].progress_map(lambda x: [token[0] for sent in x for token in sent])\n",
    "\n",
    "df.to_csv(os.path.join(DATA_DIR, \"joined_uis_all_of_march_lang_pos.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract noun and verb phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_of_speech_tag(df.Q3_pii_removed.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    cc:\n",
    "    {<CC>}\n",
    "    pronoun:\n",
    "    {<DT><IN><PRP>}\n",
    "    {<IN>?<PRP>}\n",
    "    noun_verb:\n",
    "    {<IN>?<JJ.*>*<NN.*>+<HYPH>?<VBD|VBN|VBG><NN.*>*}\n",
    "    verb:\n",
    "    {<TO><VB><IN|RP><WRB>}\n",
    "    {<IN><EX><VB.*>}\n",
    "    {<RB><TO><VB.*>+}\n",
    "    {<TO>?<VB.*><IN|WDT|WP>}\n",
    "    {<WP><VB.*>}\n",
    "    {<VB.*><RB><VB.*>*}\n",
    "    {<WDT>?<TO>?<MD|VB.*>?<RB>?<TO|IN>?<V.*>+<CC>?<V.*>*<IN|RP>?<IN>?}\n",
    "    {<MD><RB>*<VB.*>*}\n",
    "    {<VB.*><IN|TO><IN>?}\n",
    "    {<TO><VB.*><IN>+}\n",
    "    prep_noun:\n",
    "    {<IN>+<PRP\\$>?<NN><CD>?}\n",
    "    {<IN><CD><.*>}\n",
    "    {<RP>?<IN>+<JJ.*>*<NN.*>+}\n",
    "    {<IN><DT><NN.*><JJ.*>*<NN><HYPH>?<NN>}\n",
    "    {<IN><NN.*>(<HYPH>?<NN.*>)?}\n",
    "    {<JJ.*>*<IN><DT>?<NN.*>+<CD>?<NN.*>?}\n",
    "    {<IN><DT>*<JJ>?<CD>?<NN.*>+<CD>?<NN.*>?}\n",
    "    noun:\n",
    "    {<CD><NN.*>}\n",
    "    {<DT><NN.*>}\n",
    "    {<JJ.*><NN.*><CD>}\n",
    "    {<NN.*><CD><JJ.*>?}\n",
    "    {<JJ.*|NN.*><IN|TO><PRP>}\n",
    "    {<CD><NN.*><JJ.*>}\n",
    "    {<RB>*<JJ.*>*<NN.*>*}\n",
    "    {<DT><JJ.*>*<NN.*>+}\n",
    "    {<NN.*><CD>?<JJ.*>*<NN.*>*}\n",
    "    {<IN>+<CD>*<POS>*<IN>*<NN.*>}\n",
    "    {<IN><PRP\\$>?<JJ.*>*<NN.*>}\n",
    "    {<NN.*><HYPH><NN.*>}\n",
    "    {<DT>?<CD>?<JJ.*>?<CC>?<JJ.*>?<NN.*>+}\n",
    "    {<NN.*><HYPH>?<NN.*|JJ.*|VB.*>*}\n",
    "    {(<NN|NNS>|<NNP|NNPS>)<NNP|NN|NNS|NNPS>+}\n",
    "    {(<NN|NNS>+|<NNP|NNPS>+)<IN|CC>(<PRP\\$|DT><NN|NNS>+|<NNP|NNPS>+)}\n",
    "    {<JJ|RB|CD>*<NNP|NN|NNS|NNPS>+}\n",
    "    {<NNP|NN|NNS|NNPS>+}\n",
    "    adjective:\n",
    "    {<RB>*<JJ.*><CD>?}\n",
    "    \"\"\"\n",
    "\n",
    "class Chunk:\n",
    "\n",
    "    def __init__(self, label, tokens, indices):\n",
    "        self.label = label\n",
    "        self.tokens = tokens\n",
    "        self.indices = indices\n",
    "        self.text = self.text()\n",
    "        self.lemma = self.lemma()\n",
    "        self.important_lemma = self.important_lemma()\n",
    "        self.important_word = self.important_word()\n",
    "\n",
    "    def text(self):\n",
    "        return \" \".join([w for w,  _ , _  in self.tokens])\n",
    "    \n",
    "    def lemma(self):\n",
    "        return \" \".join([l for _,  _ , l  in self.tokens])\n",
    "    \n",
    "    def important_word(self):\n",
    "        return \" \".join([w for w,  pos , _  in self.tokens if re.search(r\"(NN)|(VB)|(JJ)|(DT)|(RB)|(CD)\", pos) ])\n",
    "    \n",
    "    def important_lemma(self):\n",
    "        return \" \".join([l for _,  pos , l  in self.tokens if re.search(r\"(NN)|(VB)|(JJ)|(DT)|(RB)|(CD)\", pos) ])\n",
    "    \n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "def chunk_text(tagged):\n",
    "    chunks = parser.parse(tagged)\n",
    "    index = 0\n",
    "    segments = []\n",
    "    for el in chunks:\n",
    "        if type(el) == tree.Tree:\n",
    "            chunk = Chunk(el.label(), el.leaves(), list(range(index, index + len(el.leaves()))))\n",
    "            segments.append(chunk)\n",
    "            index += len(el.leaves())\n",
    "        else:\n",
    "            index += 1\n",
    "    return segments\n",
    "\n",
    "def extract_phrase(sentences, merge_inplace=False):\n",
    "    chunks = []\n",
    "    for sentence in sentences:\n",
    "        chunks.append(chunk_text(sentence))\n",
    "    if merge_inplace:\n",
    "        return [merge_adjacent_chunks(chunk) for chunk in chunks]\n",
    "    return chunks  \n",
    "\n",
    "def merge_adjacent_chunks(chunks):\n",
    "    merged = []\n",
    "    previous_label = \"\"\n",
    "    for chunk in chunks:\n",
    "        if chunk.label == previous_label:\n",
    "            merged[-1] = Chunk(chunk.label, \n",
    "                               merged[-1].tokens + chunk.tokens, \n",
    "                               merged[-1].indices + chunk.indices)\n",
    "        else:\n",
    "            merged.append(chunk)\n",
    "        previous_label = chunk.label\n",
    "    return merged\n",
    "\n",
    "def compute_combinations(sentences, n):\n",
    "    return [chunks[i:i+n] for chunks in sentences for i in range(len(chunks)-(n-1))]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = df.iloc[7]\n",
    "example = df[df.Q3.str.contains(\"age 80\")].iloc[0]\n",
    "\n",
    "print(example.Q3)\n",
    "print()\n",
    "print(example.pos_tag)\n",
    "print()\n",
    "for sent in extract_phrase(example.pos_tag, True):\n",
    "    for chunk in sent:\n",
    "        print(\"{0:10} {1:20} {2}\".format(chunk.label.upper(), chunk.text, chunk.indices))\n",
    "    print()\n",
    "    for combo in compute_combinations([sent], 2):\n",
    "        print(f\"{combo[0].text}, {combo[1].text}\")\n",
    "    \n",
    "#     for combo in compute_combinations([sent], 3):\n",
    "#         print(f\"{combo[0].text}, {combo[1].text}, {combo[2].text}\")\n",
    "    print(\"=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute linguistic pattern combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linguistic_patterns(df_series, n):\n",
    "    pattern_dictionary = {}\n",
    "\n",
    "    for vals in tqdm_notebook(df_series.values):\n",
    "        sents = extract_phrase(vals, True)\n",
    "        \n",
    "        for combo in compute_combinations(sents, n):\n",
    "            key = tuple([c.label for c in combo])\n",
    "            counter_key =  tuple([c.text.lower() for c in combo])\n",
    "\n",
    "            if key not in pattern_dictionary.keys():\n",
    "                pattern_dictionary[key]=Counter()\n",
    "\n",
    "            pattern_dictionary[key][counter_key]+=1\n",
    "            \n",
    "    return pattern_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_d = compute_linguistic_patterns(df.pos_tag, 2)\n",
    "pattern_d.keys(), len(pattern_d)\n",
    "\n",
    "for i, (k,v) in enumerate(sorted(pattern_d.items(), \n",
    "                                 key = lambda x: len(x[1]), \n",
    "                                 reverse=True), \n",
    "                          1):\n",
    "    print(f\"{i}. {' - '.join([ks.upper() for ks in k])} : {len(v)}\\n-------------\")\n",
    "    for j, (kk,vv) in enumerate(pattern_d[k].most_common(50), 1):\n",
    "        print(f\"{j}. \\'{' '.join([f'[{kks}]' for kks in kk])}\\' : {vv}\")\n",
    "    print()\n",
    "    print(\"=======\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find specific mentions\n",
    "### Find information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(re.search(\"info|advice|guidance|look|seek|obtain\", \"i am looking for fo\", re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"a delivery slot\"\n",
    "str2 = \"online slot\"\n",
    "ratio = fuzz.ratio(str1.lower(), str2.lower())\n",
    "partial_ratio = fuzz.partial_ratio(str1.lower(), str2.lower())\n",
    "ratio, partial_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Self-identifying am/is/are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = Counter()\n",
    "for pattern in [(\"verb\", \"noun\"), (\"verb\", \"prep_noun\")]:\n",
    "    print(pattern)\n",
    "    for k,v in pattern_d[pattern].items():\n",
    "        if re.search(r\"(^a?((\\’|\\')?m))$\", k[0]):\n",
    "            identifier[k[1]] +=v\n",
    "identifier.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Play around with Fuzzywuzzy for imperfect matches (to be aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"a delivery slot\"\n",
    "str2 = \"online slot\"\n",
    "ratio = fuzz.ratio(str1.lower(), str2.lower())\n",
    "partial_ratio = fuzz.partial_ratio(str1.lower(), str2.lower())\n",
    "ratio, partial_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for identity,_ in identifier.most_common(100):\n",
    "    for identity2,_ in identifier.most_common(100):\n",
    "        if identity != identity2:\n",
    "            pr = fuzz.partial_ratio(identity.lower(), identity2.lower())\n",
    "            if pr >= 80:\n",
    "                token = fuzz.token_set_ratio(identity, identity2)\n",
    "                print(f\"{identity}, {identity2} : {pr} [{token}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get help\n",
    "\n",
    "def has_info_req(text):\n",
    "    if any([bool(re.search(\"help|assist|support\", word, re.IGNORECASE)) for word in text]):\n",
    "        return \"help\"\n",
    "    if any([bool(re.search(\"info|advice|guidance|look|seek|obtain\", word, re.IGNORECASE)) for word in text]):\n",
    "        return \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_seeking_verbs = {}\n",
    "for k,v in pattern_d[(\"verb\", \"noun\")].items():\n",
    "    if has_info_req([k[1]])!=None and has_info_req([k[1]])==\"help\":\n",
    "        print(has_info_req([k[1]]), k, v)\n",
    "#         info_seeking_verbs[k[0]]\n",
    "info_seeking_verbs.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "rowlist = []\n",
    "for key in [key for key in pattern_d.keys() ]:\n",
    "#     print(key)\n",
    "    for k,v in pattern_d[key].most_common():\n",
    "        if \"delivery\" in k[1] or \"deliver\" in k[0]:\n",
    "#             rowlist.append({f\"\"})\n",
    "            print(f\"{i}. [{k[0]}] {k[1]} : {v}\")\n",
    "            i+=1\n",
    "    print()\n",
    "    i=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_triples = compute_linguistic_patterns(df.pos_tag, 3)\n",
    "\n",
    "for i, (k,v) in enumerate(sorted(pd_triples.items(), \n",
    "                                 key = lambda x: len(x[1]), \n",
    "                                 reverse=True), \n",
    "                          1):\n",
    "    print(f\"{i}. {' - '.join([ks.upper() for ks in k])} : {len(v)}\\n-------------\")\n",
    "    for j, (args, counts) in enumerate(pd_triples[k].most_common(10), 1):\n",
    "        print(f\"{j}. {args}: {counts}\")\n",
    "    print()\n",
    "    print(\"=======\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute `arg1` - `arg2` co-occurrence db - couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_db = {}\n",
    "\n",
    "for vals in tqdm_notebook(df.pos_tag.values):\n",
    "    sents = extract_phrase(vals, True)\n",
    "    for combo in compute_combinations(sents, 2):\n",
    "        key = (combo[0].label, combo[1].label)\n",
    "        arg1 = combo[0].important_word.lower()\n",
    "        arg2 = combo[1].text.lower()\n",
    "        \n",
    "        if key not in pattern_db.keys():\n",
    "            pattern_db[key] = {}\n",
    "        if arg1 not in pattern_db[key].keys():\n",
    "            pattern_db[key][arg1] = Counter()\n",
    "            \n",
    "        pattern_db[key][arg1][arg2]+=1\n",
    "\n",
    "print(f\"There are {len(pattern_db)} possible grammatical combos.\")\n",
    "for i, (k,v) in enumerate(sorted(pattern_db.items(),\n",
    "                         key = lambda x: len(x[1].values()),\n",
    "                         reverse= True)[0:15],\n",
    "                                 1):\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet categorization of individual arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(pos):\n",
    "    if pos.startswith(\"NN\"):\n",
    "        return wn.NOUN\n",
    "    if pos.startswith(\"VB\"):\n",
    "        return wn.VERB\n",
    "    \n",
    "def wordnet_category(word, pos):\n",
    "    wn_pos = get_wordnet_pos(pos)\n",
    "    if len(word.split(\" \")) > 1:\n",
    "        word = word.split(\" \")[-1]\n",
    "    if len(wn.synsets(word, wn_pos))>0 :\n",
    "        syn = wn.synsets(word, wn_pos)[0]\n",
    "        return syn.lexname()\n",
    "    return \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_category(\"no one\", \"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_compute_categories(argument_list):\n",
    "    counter = Counter()\n",
    "    for argument, counts in argument_list.items():\n",
    "        wordnet_cat = wordnet_category(argument, \"NN\")\n",
    "        if \"Tops\" in wordnet_cat:\n",
    "            wordnet_cat = f\"noun.{argument.lower().split(' ')[-1]}\"\n",
    "        counter[wordnet_cat] += counts\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ('verb', 'noun')\n",
    "print(f\"There are {len(pattern_db[key])} {key[0]}s, accompanied by {key[1]}s.\")\n",
    "for i, (arg1, arg2) in enumerate(sorted(pattern_db[key].items(),\n",
    "                         key = lambda x: sum(x[1].values()),\n",
    "                         reverse= True)[:50],\n",
    "                                 1):\n",
    "    print(f\"{i}. {arg1} :: {sum(arg2.values())} \\n-----------\")\n",
    "    print(bulk_compute_categories(arg2).most_common(10))\n",
    "    for j, (arg2_val, arg2_counts) in enumerate(arg2.most_common(20), 1):\n",
    "        print(f\"{j}. {arg2_val} : {arg2_counts}\")\n",
    "        print(wordnet_category(arg2_val, \"NN\"))\n",
    "    print(\"=======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
