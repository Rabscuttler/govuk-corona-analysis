{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import src.utils.regex as regex\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import hdbscan\n",
    "! spacy download en_core_web_lg\n",
    "import spacy\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "STEMMER = PorterStemmer()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of entries contain questions about travel advice, often with individual country names\n",
    "this meant the clusterer was clustering by country name which wasn't ideal\n",
    "The same goes for months etc, so they are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')\n",
    "def remove_common_terms(text):\n",
    "    doc = model(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\" or ent.label_ == \"DATE\":\n",
    "            text = text.replace(ent.text, ent.label_)\n",
    "    return text\n",
    "\n",
    "# Sanity check\n",
    "remove_common_terms(\"to find out an update for my holiday in mexico in april\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data, there is a lot going on here, explained in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "df = pd.read_csv (\"../data/raw/joined_uis_all_of_march.csv\")\n",
    "q3 = \"Q3\"\n",
    "df['q3_copy'] = df[q3]\n",
    "\n",
    "corona_slugs = open('../data/raw/coronavirus_page_slugs.txt').read().split(\"\\n\")\n",
    "corona_related_items_regex = regex.coronavirus_misspellings_and_typos_regex() + '|sick pay|ssp|sick|isolation|closures|quarantine|closure|cobra|cruise|hand|isolat|older people|pandemic|school|social distancing|symptoms|cases|travel|wuhan|care|elderly|care home|carehome'\n",
    "\n",
    "# These are terms that are functionally the same but people use different terms, this standardises them\n",
    "same_terms = {\n",
    "    \"travelling\": \"travel\",\n",
    "    \"travellers\": \"travel\",\n",
    "    \"holiday\": \"travel\",\n",
    "    \"self-isolation\": \"quarantine\",\n",
    "    \"selfisolation\": \"quarantine\",\n",
    "    \"self isolation\": \"quarantine\",\n",
    "    \"isolation\": \"quarantine\",\n",
    "    \"statuatory sick pay\": \"ssp\",\n",
    "    \"sick pay\": \"ssp\",\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # We'll be removing non alphabetical characters but we want to keep the non emergency phone number \n",
    "    # '111' in, so we'll just replace that with text\n",
    "    text = text.replace(\"111\", \"oneoneone\")\n",
    "    # Same for 999\n",
    "    text = text.replace(\"999\", \"nineninenine\")\n",
    "    # Remove non alphabetical or space characters\n",
    "    text = re.sub(\"[^a-zA-Z\\s:]\", \"\", text)\n",
    "    # Use our function from previous cell\n",
    "    text = remove_common_terms(text)\n",
    "    # This is done after remove_common_terms because spacy doesn't \n",
    "    # always recognise country names without a capital letter at the beginning!\n",
    "    text = text.lower()\n",
    "    text = re.sub(regex.coronavirus_misspellings_and_typos_regex() + \"|virus\", \"\", text)\n",
    "    # People using different terms for \"I want to know\", so just remove those\n",
    "    text = re.sub(\"wanted to find out|to look up about|to get an update|to find infos|to find info|to find out|to understand|to read the|check on advice|to check|ti get advice|to get advice|for information on\", \"\", text)\n",
    "    for word_to_replace, word_to_replace_with in same_terms.items():\n",
    "        text.replace(word_to_replace, word_to_replace_with)\n",
    "    return text\n",
    "\n",
    "df[q3] = df[q3].apply(clean_text)\n",
    "\n",
    "# Remove rows without a page sequence\n",
    "df = df[df['PageSequence'].notnull()].reset_index(drop=True)\n",
    "\n",
    "# We only want to cluster rows that are relevant to corona stuff\n",
    "# so we have the column 'has_corona_page'\n",
    "# It is only true if they have visted a corona page AND included a relevant term in the feedback\n",
    "# (there was some irrelevant stuff about passports, we may want to remove the need for a relevant term\n",
    "# as people may be using terms not in that list and we might miss out on some insights)\n",
    "for index, row in df.iterrows():\n",
    "    has_corona_page = False\n",
    "    if re.search(corona_related_items_regex, df.at[index, q3]) is not None:\n",
    "        for slug in row['PageSequence'].split(\">>\"):\n",
    "            if slug in corona_slugs or \"coronavirus\" in slug:\n",
    "                has_corona_page = True\n",
    "    df.at[index, 'has_corona_page'] = has_corona_page\n",
    "df = df[df['has_corona_page']].reset_index(drop=True)\n",
    "\n",
    "# Remove duplicate users\n",
    "df = df.drop_duplicates('intents_clientID')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    return [STEMMER.stem(item) for item in tokens]\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_clustering(df, previous_cluster_level, previous_cluster_label, cluster_label):\n",
    "    count = df[df[previous_cluster_label] == cluster_label].shape[0]\n",
    "    # Probably no point getting more granular than this\n",
    "    if count < 10:\n",
    "        return df\n",
    "    # Try to automatically get the a good minimum cluster size.\n",
    "    # I _think_ there is a lot of potential to improve this, I've done\n",
    "    # similar things before with automatically trying a few values\n",
    "    # and selecting the best on some metric. This is a bit basic\n",
    "    min_cluster_size = int(count / 40)\n",
    "    if min_cluster_size < 2:\n",
    "        min_cluster_size = 2\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenize, analyzer='word', stop_words=stopwords.words('english'), max_features=100, ngram_range=(1,3) )\n",
    "    X = vectorizer.fit_transform(df[df[previous_cluster_label] == cluster_label][q3]).toarray()\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,min_samples=2, cluster_selection_method='leaf')\n",
    "    clusterer.fit(X)\n",
    "    # _labels is the cluster they've been assigned to\n",
    "    level = previous_cluster_level + 1\n",
    "    new_cluster_label = f\"cluster_{previous_cluster_label}_{level}_{cluster_label}\"\n",
    "    df[new_cluster_label] = \"\"\n",
    "    label_index = 0\n",
    "    for index, row in df[df[previous_cluster_label] == cluster_label].iterrows():\n",
    "        df.at[index, new_cluster_label] = clusterer.labels_[label_index]\n",
    "        # This is the probability score for this specific cluster. Can be used when presenting\n",
    "        # the output, as items with the highest score belong more to that cluster, so if we sort\n",
    "        # by this column and select the rows with the highest score, those will be the 'best'\n",
    "        # examples for this cluster\n",
    "        df.at[index, f\"{new_cluster_label}_probability\"] = clusterer.probabilities_[label_index]\n",
    "        label_index += 1\n",
    "    for label in clusterer.labels_:\n",
    "        # -1 is can't be clustered \n",
    "        if label >= 0:\n",
    "            df = recursive_clustering(df, level, new_cluster_label, label)\n",
    "    return df\n",
    "    \n",
    "df[\"cluster_0\"] = 0\n",
    "df[\"probabilities\"] = \"\"\n",
    "df = recursive_clustering(df, 0, \"cluster_0\", 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output clusters\n",
    "\n",
    "This would better be done in a tree format, which will also allow for exploration but didn't have time. This is a good example of what I'm thinking (and should be easy to implement) https://bl.ocks.org/d3noob/8375092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns with 'cluster' in the title, those are the ones generated in the last \n",
    "cluster_columns = []\n",
    "for column in df.columns:\n",
    "    if \"cluster\" in column and not \"probability\" in column:\n",
    "        cluster_columns.append(column)\n",
    "\n",
    "def cluster_name(column, group):\n",
    "    return column.replace(\"cluster_\", \"\") + str(group)\n",
    "        \n",
    "def get_parent_cluster_name(column, group):\n",
    "    column = cluster_name(column, group)\n",
    "    lengths = {}\n",
    "    for other_column in cluster_columns:\n",
    "        other_column = cluster_name(other_column, \"\")\n",
    "        if other_column != column and len(other_column) < len(column):\n",
    "            regex = '[' + other_column + ']'          \n",
    "            matches = re.findall(other_column, column)\n",
    "            if any(matches):\n",
    "                lengths[other_column] = len(matches[0])\n",
    "    sorted_lengths_of_matches = list({k: v for k, v in sorted(lengths.items(), reverse= True, key=lambda item: item[1])}.keys())\n",
    "    if any(sorted_lengths_of_matches):\n",
    "        return sorted_lengths_of_matches[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "clusters = []\n",
    "for col in cluster_columns:\n",
    "    unique_groups = list(set(df[col]))\n",
    "    for group in unique_groups:\n",
    "        this_cluster = df.copy()\n",
    "        this_cluster = this_cluster[this_cluster[col] == group].reset_index(drop = True)\n",
    "        if col != \"cluster_0\":\n",
    "            df.sort_values(by=[col + '_probability'], inplace=True, ascending=False)\n",
    "        if this_cluster.shape[0] >= 3:\n",
    "            cluster = {}\n",
    "            cluster['cluster_name'] = cluster_name(col, group)\n",
    "            cluster['parent_cluster_name'] = get_parent_cluster_name(col, \"\")\n",
    "            num_examples = this_cluster.shape[0]\n",
    "            example_one = this_cluster.at[0, q3]\n",
    "            example_two = this_cluster.at[1, q3]\n",
    "            example_three = this_cluster.at[2, q3]\n",
    "            name = f\"Num entries: {num_examples}\\n{example_one}\\n{example_two}\\n{example_three}\"\n",
    "            cluster['name'] = name\n",
    "            cluster['num_examples'] = num_examples\n",
    "            cluster['example_one'] = example_one.replace(\"\\n\", \"\")\n",
    "            cluster['example_two'] = example_two.replace(\"\\n\", \"\")\n",
    "            cluster['example_three'] = example_three.replace(\"\\n\", \"\")\n",
    "            cluster['children'] = []\n",
    "            clusters.append(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tree structure\n",
    "for cluster in clusters:\n",
    "    parent_cluster_name = cluster['cluster_name']\n",
    "    for other_cluster in clusters:\n",
    "        if other_cluster['parent_cluster_name'] == parent_cluster_name:\n",
    "            cluster_is_in_children = False\n",
    "            for child_cluster in cluster['children']:\n",
    "                if child_cluster['cluster_name'] == other_cluster['cluster_name']:\n",
    "                    cluster_is_in_children = True\n",
    "            if cluster_is_in_children == False:\n",
    "                cluster['children'].append(other_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the tree work in d3 is a good idea for the future but it was quite a lot of work and I didn't really have the time, so here is a way to spit it out as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_addition(depth, cluster):\n",
    "    commas = \"\"\n",
    "    for i in range(depth):\n",
    "        commas += \",\"\n",
    "    title = commas + f\"Cluster with {cluster['num_examples']} entries - top 3:\"\n",
    "    example_one = commas + cluster['example_one']\n",
    "    example_two = commas + cluster['example_two']\n",
    "    example_three = commas + cluster['example_three']\n",
    "    row = commas + \"\\n\"\n",
    "    row += title + \"\\n\"\n",
    "    row += example_one + \"\\n\"\n",
    "    row += example_two + \"\\n\"\n",
    "    row += example_three + \"\\n\"\n",
    "    for child in cluster['children']:\n",
    "        row += recursive_addition(depth + 1, child)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = \"\"\n",
    "level = 0\n",
    "level_depths = {}\n",
    "for cluster in clusters:\n",
    "    csv += recursive_addition(1, cluster)\n",
    "\n",
    "text_file = open(\"Output.csv\", \"w\")\n",
    "text_file.write(csv)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
