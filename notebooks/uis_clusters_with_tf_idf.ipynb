{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import src.utils.regex as regex\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import ktrain\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import hdbscan\n",
    "! spacy download en_core_web_lg\n",
    "import spacy\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "STEMMER = PorterStemmer()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of entries contain questions about travel advice, often with individual country names\n",
    "this meant the clusterer was clustering by country name which wasn't ideal\n",
    "The same goes for months etc, so they are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')\n",
    "def remove_common_terms(text):\n",
    "    doc = model(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\" or ent.label_ == \"DATE\":\n",
    "            text = text.replace(ent.text, ent.label_)\n",
    "    return text\n",
    "\n",
    "# Sanity check\n",
    "remove_common_terms(\"to find out an update for my holiday in mexico in april\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data, there is a lot going on here, explained in the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "df = pd.read_csv (\"../data/raw/uis_ga_all_cols_20200301_20200327.csv\")\n",
    "q3 = \"Q3\"\n",
    "df['q3_copy'] = df[q3]\n",
    "\n",
    "corona_slugs = open('../data/raw/coronavirus_page_slugs.txt').read().split(\"\\n\")\n",
    "corona_related_items_regex = regex.coronavirus_misspellings_and_typos_regex() + '|sick pay|ssp|sick|isolation|closures|quarantine|closure|cobra|cruise|hand|isolat|older people|pandemic|school|social distancing|symptoms|cases|travel|wuhan|care|elderly|care home|carehome'\n",
    "\n",
    "# These are terms that are functionally the same but people use different terms, this standardises them\n",
    "same_terms = {\n",
    "    \"travelling\": \"travel\",\n",
    "    \"travellers\": \"travel\",\n",
    "    \"holiday\": \"travel\",\n",
    "    \"self-isolation\": \"quarantine\",\n",
    "    \"selfisolation\": \"quarantine\",\n",
    "    \"self isolation\": \"quarantine\",\n",
    "    \"isolation\": \"quarantine\",\n",
    "    \"statuatory sick pay\": \"ssp\",\n",
    "    \"sick pay\": \"ssp\",\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # We'll be removing non alphabetical characters but we want to keep the non emergency phone number \n",
    "    # '111' in, so we'll just replace that with text\n",
    "    text = text.replace(\"111\", \"oneoneone\")\n",
    "    # Same for 999\n",
    "    text = text.replace(\"999\", \"nineninenine\")\n",
    "    # Remove non alphabetical or space characters\n",
    "    text = re.sub(\"[^a-zA-Z\\s:]\", \"\", text)\n",
    "    # Use our function from previous cell\n",
    "    text = remove_common_terms(text)\n",
    "    # This is done after remove_common_terms because spacy doesn't \n",
    "    # always recognise country names without a capital letter at the beginning!\n",
    "    text = text.lower()\n",
    "    text = re.sub(regex.coronavirus_misspellings_and_typos_regex() + \"|virus\", \"\", text)\n",
    "    # People using different terms for \"I want to know\", so just remove those\n",
    "    text = re.sub(\"wanted to find out|to look up about|to get an update|to find infos|to find info|to find out|to understand|to read the|check on advice|to check|ti get advice|to get advice|for information on\", \"\", text)\n",
    "    for word_to_replace, word_to_replace_with in same_terms.items():\n",
    "        text.replace(word_to_replace, word_to_replace_with)\n",
    "    return text\n",
    "\n",
    "df[q3] = df[q3].apply(clean_text)\n",
    "\n",
    "# Remove rows without a page sequence\n",
    "df = df[df['PageSequence'].notnull()].reset_index(drop=True)\n",
    "\n",
    "# We only want to cluster rows that are relevant to corona stuff\n",
    "# so we have the column 'has_corona_page'\n",
    "# It is only true if they have visted a corona page AND included a relevant term in the feedback\n",
    "# (there was some irrelevant stuff about passports, we may want to remove the need for a relevant term\n",
    "# as people may be using terms not in that list and we might miss out on some insights)\n",
    "for index, row in df.iterrows():\n",
    "    has_corona_page = False\n",
    "    if re.search(corona_related_items_regex, df.at[index, q3]) is not None:\n",
    "        for slug in row['PageSequence'].split(\">>\"):\n",
    "            if slug in corona_slugs or \"coronavirus\" in slug:\n",
    "                has_corona_page = True\n",
    "    df.at[index, 'has_corona_page'] = has_corona_page\n",
    "df = df[df['has_corona_page']].reset_index(drop=True)\n",
    "\n",
    "# Remove duplicate users\n",
    "df = df.drop_duplicates('intents_clientID')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    return [STEMMER.stem(item) for item in tokens]\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, analyzer='word', stop_words=stopwords.words('english'), max_features=100, nngram_range=(1,3) )\n",
    "X = vectorizer.fit_transform(df[q3]).toarray()\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50,min_samples=2, cluster_selection_method='leaf')\n",
    "clusterer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is helpful for debugging\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _labels is the cluster they've been assigned to\n",
    "df['cluster'] = clusterer.labels_\n",
    "# _probabilities, from the comment in the library:\n",
    "# Cluster membership strengths for each point. Noisy samples are assigned 0.\n",
    "df['probabilities'] = clusterer.probabilities_\n",
    "df.sort_values(by=['probabilities'], inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df[df['cluster'] == -1].shape[0]} items out of {df.shape[0]} couldn't be put into a cluster\")\n",
    "\n",
    "def words_in_entry(entry):\n",
    "      words = entry.split(\" \")\n",
    "      words = [word if word not in stopwords.words('english') else \"\" for word in words]\n",
    "      words = filter(lambda word: len(word) > 0, words)\n",
    "      return words\n",
    "     \n",
    "def most_common_words_in_cluster(cluster_entries):\n",
    "      feature_words = vectorizer.get_feature_names()\n",
    "      common_words = []\n",
    "      for sentence in cluster_entries[q3]:\n",
    "          for word in sentence.split(\" \"):\n",
    "              tokenized_word = tokenize(word)\n",
    "              for word in tokenized_word:\n",
    "                    # The line commented out here should work but I haven't tested it \n",
    "                    # and don't want to hand over a script that doesn't work\n",
    "                  if word in feature_words:# and word not in stopwords.words('english'):\n",
    "                      common_words.append(word)\n",
    "      return Counter(common_words).most_common(10)\n",
    "\n",
    "\n",
    "errors = []\n",
    "for topic in list(set(clusterer.labels_)):\n",
    "    # Topic -1 is the cluster for things that don't really have a home\n",
    "    if topic > -1:\n",
    "        # TODO: Find a better way of doing this!\n",
    "        df_copy = df.copy()\n",
    "        cluster_entries = df_copy[df_copy['cluster'] == topic].reset_index(drop = True)\n",
    "        cluster_count = len(cluster_entries)\n",
    "        print(cluster_count)\n",
    "        if cluster_count > 0:\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "            print(f\"New cluster with {cluster_count} entries\")\n",
    "            print(f\"Most common words are: {most_common_words_in_cluster(cluster_entries)}\")\n",
    "            print(f\"1. {cluster_entries.at[0, 'q3_copy']}\")\n",
    "            print(f\"2. {cluster_entries.at[1, 'q3_copy']}\")\n",
    "            print(f\"3. {cluster_entries.at[2, 'q3_copy']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
